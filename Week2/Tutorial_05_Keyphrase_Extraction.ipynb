{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tutorial 05 - Keyphrase Extraction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8cn0BmU7EOZ"
      },
      "source": [
        "# Tutorial 5: Keyphrase Extraction\n",
        "\n",
        "**Keyphrase extraction** is the task of automatically selecting a small set of phrases that best describe a given free text document.\n",
        "\n",
        "Keyphrase extraction, also known as terminology extraction is defined as the process or technique of extracting key important and relevant terms or phrases from a body of unstructured text such that the core topics or themes of the text document(s) are captured in these key phrases. This technique falls under the broad umbrella of information retrieval and extraction. \n",
        "\n",
        "_Supervised_ keyphrase extraction requires large amounts of labeled training data and generalizes very poorly outside the domain of the training data. At the same time, _unsupervised_ systems have issues with accuracy, and often do not generalize well, as they require the input document to belong to a larger corpus also given as input.\n",
        "\n",
        "In this notebook, we will cover keyphrase extraction methods based on:\n",
        "\n",
        "+ Collocations\n",
        "+ N-grams\n",
        "+ Weighted Tag Based methods\n",
        "+ RAKE (Rapid Automatic Keyword Extraction algorithm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4IM_Dp27QPY"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yy1ec1ZXZVCC",
        "outputId": "6a691668-39e6-4016-e801-3c212b85bf16"
      },
      "source": [
        "!pip install contractions\n",
        "!pip install textsearch"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: contractions in /usr/local/lib/python3.6/dist-packages (0.0.45)\n",
            "Requirement already satisfied: textsearch in /usr/local/lib/python3.6/dist-packages (from contractions) (0.0.17)\n",
            "Requirement already satisfied: Unidecode in /usr/local/lib/python3.6/dist-packages (from textsearch->contractions) (1.1.2)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.6/dist-packages (from textsearch->contractions) (1.4.0)\n",
            "Requirement already satisfied: textsearch in /usr/local/lib/python3.6/dist-packages (0.0.17)\n",
            "Requirement already satisfied: Unidecode in /usr/local/lib/python3.6/dist-packages (from textsearch) (1.1.2)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.6/dist-packages (from textsearch) (1.4.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWFYXihuabq8"
      },
      "source": [
        "import nltk"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSF8h06gadGu",
        "outputId": "0a905330-3075-42c5-af70-be51b3ba33c8"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CiCrtZo9Ydoy",
        "outputId": "e18ed2e7-4be2-4069-b237-7826fb27ab40"
      },
      "source": [
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gX1xHXN9YvRu"
      },
      "source": [
        "## Basic Text Pre-processor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IP3l-MS3Y1LF"
      },
      "source": [
        "import re\r\n",
        "stop_words = nltk.corpus.stopwords.words('english')\r\n",
        "\r\n",
        "def normalize_document(doc):\r\n",
        "    # lower case and remove special characters\\whitespaces\r\n",
        "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, flags=re.I|re.A)\r\n",
        "    doc = doc.lower()\r\n",
        "    doc = doc.strip()\r\n",
        "    # tokenize document\r\n",
        "    tokens = nltk.word_tokenize(doc)\r\n",
        "    # filter stopwords out of document\r\n",
        "    filtered_tokens = [token for token in tokens if token not in stop_words]\r\n",
        "    # re-create document from filtered tokens\r\n",
        "    doc = ' '.join(filtered_tokens)\r\n",
        "    return doc"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6nnRXJo68ou"
      },
      "source": [
        "from nltk.corpus import gutenberg\n",
        "from operator import itemgetter"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EKCla8C7OU5"
      },
      "source": [
        "## Load Dataset\n",
        "\n",
        "In this notebook, we will make use of a sample description of elephants taken from Wikipedia.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kC7TETxTlXhd"
      },
      "source": [
        "import requests\r\n",
        "import bs4\r\n",
        "import re\r\n",
        "\r\n",
        "res = requests.get('https://pastebin.com/raw/W182iruJ')\r\n",
        "data = res.text"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmDILxUW7LRs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "376f6c9f-3395-4076-890b-ada727691ed6"
      },
      "source": [
        "elephants = nltk.sent_tokenize(data)\n",
        "elephants[:10]"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Elephants are the largest living land mammals.',\n",
              " 'The largest elephant recorded was one shot in Angola, 1974.',\n",
              " 'It weighed 27,060 pounds (13.5 tons) and stood 13 feet 8 inches tall.',\n",
              " 'Their skin color is grey.',\n",
              " 'At birth, an elephant calf may weigh as much as 100 kg (225 pounds).',\n",
              " 'The baby elephant develops for 20 to 22 months inside its mother.',\n",
              " 'No other land animal takes this long to develop before being born.',\n",
              " 'In the wild, elephants have strong family relationship.',\n",
              " 'Their ways of acting toward other elephants are hard for people to understand.',\n",
              " 'They \"talk\" to each other with very low sounds.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfaCSZJQZImA",
        "outputId": "42f17d86-d385-409a-a9cd-fba88c26e516"
      },
      "source": [
        "norm_elephants = list(filter(None, [normalize_document(line)\r\n",
        "                                      for line in elephants]))\r\n",
        "norm_elephants[:10]"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['elephants largest living land mammals',\n",
              " 'largest elephant recorded one shot angola',\n",
              " 'weighed pounds tons stood feet inches tall',\n",
              " 'skin color grey',\n",
              " 'birth elephant calf may weigh much kg pounds',\n",
              " 'baby elephant develops months inside mother',\n",
              " 'land animal takes long develop born',\n",
              " 'wild elephants strong family relationship',\n",
              " 'ways acting toward elephants hard people understand',\n",
              " 'talk low sounds']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIrCx6uU7XUT"
      },
      "source": [
        "## Collocations\n",
        "\n",
        "A collocation can be defined as a sequence or group of words which tend to occur frequently such that this frequency tends to be more than what could be termed as a random or chance occurrence. \n",
        "\n",
        "There are various ways to extract collocations and one of the best ways to do that is to use an n-gram grouping or segmentation approach where we construct n-grams out of a corpus and then counting the frequency of each n-gram and ranking them based on their frequency of occurrence to get the most frequent n-gram collocations. \n",
        "\n",
        "Thus collocations are phrases or expressions containing multiple words, that are highly likely to co-occur. For example — ‘social media’, ‘school holiday’, ‘machine learning’, etc.\n",
        "\n",
        "Let us prepare a function to generate n-grams from a sequence of tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTKaXycj7TvZ"
      },
      "source": [
        "def compute_ngrams(sequence, n):\n",
        "    return list(\n",
        "            zip(*(sequence[index:] \n",
        "                     for index in range(n)))\n",
        "    )"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFNbtVFm7nxB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "068fda77-15f7-4fe3-a5da-a366e1bfd060"
      },
      "source": [
        "# test bi-gram extraction\n",
        "compute_ngrams([1,2,3,4], 2)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1, 2), (2, 3), (3, 4)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qW0Ig0vP7p-j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03553281-a92c-4907-82fc-9868a00b2905"
      },
      "source": [
        "# test tri-gram extraction\n",
        "compute_ngrams([1,2,3,4], 3)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1, 2, 3), (2, 3, 4)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iv2GhqH37xpO"
      },
      "source": [
        "## N-Grams\n",
        "\n",
        "In the fields of computational linguistics and probability, an n-gram is a contiguous sequence of n items from a given sample of text or speech. The items can be phonemes, syllables, letters, words or base pairs according to the application. The n-grams typically are collected from a text or speech corpus.\n",
        "\n",
        "Lets find some top used ngrams from our dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQblHbtD7skH"
      },
      "source": [
        "def flatten_corpus(corpus):\n",
        "    return ' '.join([document.strip() \n",
        "                     for document in corpus])"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5G6PsmgI7zCz"
      },
      "source": [
        "def get_top_ngrams(corpus, ngram_val=1, limit=5):\n",
        "\n",
        "    corpus = flatten_corpus(corpus)\n",
        "    tokens = nltk.word_tokenize(corpus)\n",
        "\n",
        "    ngrams = compute_ngrams(tokens, ngram_val)\n",
        "    ngrams_freq_dist = nltk.FreqDist(ngrams)\n",
        "    sorted_ngrams_fd = sorted(ngrams_freq_dist.items(), \n",
        "                              key=itemgetter(1), reverse=True)\n",
        "    sorted_ngrams = sorted_ngrams_fd[0:limit]\n",
        "    sorted_ngrams = [(' '.join(text), freq) \n",
        "                     for text, freq in sorted_ngrams]\n",
        "\n",
        "    return sorted_ngrams"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0re6Rw1TaHaq"
      },
      "source": [
        "We make use of nltk's FreqDist class to create a counter of all the n-grams based on their frequency and then we sort them based on their frequency and return the top n-grams based on the specified user limit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVQXeyPx7zt4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b6ff65c-a08f-4ea2-bfb9-0bc334255343"
      },
      "source": [
        "get_top_ngrams(corpus=norm_elephants, ngram_val=2,\n",
        "               limit=20)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('african elephants', 8),\n",
              " ('asian elephants', 6),\n",
              " ('modern elephants', 4),\n",
              " ('indian elephants', 3),\n",
              " ('elephants used', 3),\n",
              " ('elephants strong', 2),\n",
              " ('elephants african', 2),\n",
              " ('loxodonta africanus', 2),\n",
              " ('elephants eat', 2),\n",
              " ('teeth called', 2),\n",
              " ('group loxodonta', 2),\n",
              " ('evolved gomphotheres', 2),\n",
              " ('used tourists', 2),\n",
              " ('south africa', 2),\n",
              " ('female elephant', 2),\n",
              " ('elephant often', 2),\n",
              " ('elephants largest', 1),\n",
              " ('largest living', 1),\n",
              " ('living land', 1),\n",
              " ('land mammals', 1)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDjirySo73i2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a946d05b-c931-43dc-be2f-a49c7260646e"
      },
      "source": [
        "get_top_ngrams(corpus=norm_elephants, ngram_val=3,\n",
        "               limit=20)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('indian elephants eat', 2),\n",
              " ('elephants used tourists', 2),\n",
              " ('elephants largest living', 1),\n",
              " ('largest living land', 1),\n",
              " ('living land mammals', 1),\n",
              " ('land mammals largest', 1),\n",
              " ('mammals largest elephant', 1),\n",
              " ('largest elephant recorded', 1),\n",
              " ('elephant recorded one', 1),\n",
              " ('recorded one shot', 1),\n",
              " ('one shot angola', 1),\n",
              " ('shot angola weighed', 1),\n",
              " ('angola weighed pounds', 1),\n",
              " ('weighed pounds tons', 1),\n",
              " ('pounds tons stood', 1),\n",
              " ('tons stood feet', 1),\n",
              " ('stood feet inches', 1),\n",
              " ('feet inches tall', 1),\n",
              " ('inches tall skin', 1),\n",
              " ('tall skin color', 1)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csgL36UM8f1W"
      },
      "source": [
        "##N-Gram and Pointwise Mutual Information\n",
        "\n",
        "The collocations package from ``nltk`` provides collocation finders which by default consider all ngrams in a text as candidate collocations.\n",
        "\n",
        "\n",
        "### Point Wise Mutual Information\n",
        "Simple frequency isn’t the best measure of association between words. One problem is that raw frequency is very skewed and not very discriminative. If we want to know what kinds of contexts are shared by apricot and pineapple but not by digital and information, we’re not going to get good discrimination from words like the, it, or they, which occur frequently with all sorts of words and aren’t informative about any particular word.\n",
        "\n",
        "Pointwise mutual information can be computed for two events or terms as the logarithm of the ratio of the probability of them occurring together by the product of their individual probabilities assuming that they are independent of each other \n",
        "\n",
        "For more details, [Refer Here](https://web.stanford.edu/~jurafsky/slp3/15.pdf) and [here](https://eranraviv.com/understanding-pointwise-mutual-information-in-statistics/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IpDTScq8ihS"
      },
      "source": [
        "from nltk.collocations import BigramCollocationFinder\n",
        "from nltk.collocations import BigramAssocMeasures"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rd4K79tp8kj8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4de59aa2-2af8-4c25-90f8-492d8bd29a81"
      },
      "source": [
        "finder = BigramCollocationFinder.from_documents([item.split() \n",
        "                                                for item \n",
        "                                                in norm_elephants])\n",
        "finder"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<nltk.collocations.BigramCollocationFinder at 0x7fdd9ada6ac8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9oPj17T8m0_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a3b23dd-b9d8-42ac-f93e-aab4cdf1ce4b"
      },
      "source": [
        "bigram_measures = BigramAssocMeasures()                                                \n",
        "finder.nbest(bigram_measures.raw_freq, 20)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('african', 'elephants'),\n",
              " ('asian', 'elephants'),\n",
              " ('modern', 'elephants'),\n",
              " ('elephants', 'used'),\n",
              " ('indian', 'elephants'),\n",
              " ('elephant', 'often'),\n",
              " ('elephants', 'eat'),\n",
              " ('elephants', 'strong'),\n",
              " ('evolved', 'gomphotheres'),\n",
              " ('female', 'elephant'),\n",
              " ('group', 'loxodonta'),\n",
              " ('loxodonta', 'africanus'),\n",
              " ('south', 'africa'),\n",
              " ('teeth', 'called'),\n",
              " ('used', 'tourists'),\n",
              " ('acacia', 'trees'),\n",
              " ('across', 'alps'),\n",
              " ('acting', 'toward'),\n",
              " ('actual', 'family'),\n",
              " ('africa', 'asia')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yvkaIhC8pH1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa16c88f-07da-4e1a-ecbf-3d47c888b5b3"
      },
      "source": [
        "finder.nbest(bigram_measures.pmi, 20)   "
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('across', 'alps'),\n",
              " ('alive', 'feeding'),\n",
              " ('allows', 'restricted'),\n",
              " ('ancestors', 'palaeocene'),\n",
              " ('ants', 'bite'),\n",
              " ('appendix', 'ii'),\n",
              " ('avoid', 'acacia'),\n",
              " ('became', 'cooler'),\n",
              " ('cameroon', 'gabon'),\n",
              " ('carthaginian', 'general'),\n",
              " ('climate', 'became'),\n",
              " ('color', 'grey'),\n",
              " ('concentration', 'silica'),\n",
              " ('conservation', 'efforts'),\n",
              " ('controlled', 'contraception'),\n",
              " ('cooler', 'drier'),\n",
              " ('countries', 'sport'),\n",
              " ('country', 'found'),\n",
              " ('crushed', 'criminals'),\n",
              " ('dietary', 'supply')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4dnTypK8tNu"
      },
      "source": [
        "from nltk.collocations import TrigramCollocationFinder\n",
        "from nltk.collocations import TrigramAssocMeasures"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dAKIxCE8uh6"
      },
      "source": [
        "finder = TrigramCollocationFinder.from_documents([item.split() \n",
        "                                                for item \n",
        "                                                in norm_elephants])"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LatLxN_x9Ai6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93ea6de6-5fa5-4acc-b5c6-62f10c00ea4d"
      },
      "source": [
        "trigram_measures = TrigramAssocMeasures()                                                \n",
        "finder.nbest(trigram_measures.raw_freq, 20)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('elephants', 'used', 'tourists'),\n",
              " ('indian', 'elephants', 'eat'),\n",
              " ('acacia', 'trees', 'symbiotic'),\n",
              " ('across', 'alps', 'fought'),\n",
              " ('acting', 'toward', 'elephants'),\n",
              " ('actual', 'family', 'elephantidae'),\n",
              " ('africa', 'asia', 'hard'),\n",
              " ('africa', 'tanzania', 'zambia'),\n",
              " ('african', 'asian', 'elephants'),\n",
              " ('african', 'elephant', 'kind'),\n",
              " ('african', 'elephants', 'cool'),\n",
              " ('african', 'elephants', 'larger'),\n",
              " ('african', 'elephants', 'live'),\n",
              " ('african', 'elephants', 'low'),\n",
              " ('african', 'elephants', 'receive'),\n",
              " ('african', 'elephants', 'tusks'),\n",
              " ('african', 'elephants', 'two'),\n",
              " ('african', 'loxodonta', 'africanus'),\n",
              " ('africanus', 'asian', 'elephants'),\n",
              " ('alive', 'feeding', 'soft')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4c06dhcfKc4u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d341608-18b1-4bfb-ab45-d6c86cad49c7"
      },
      "source": [
        "finder.nbest(trigram_measures.pmi, 20)  "
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('ancestors', 'palaeocene', 'eocene'),\n",
              " ('appendix', 'ii', 'status'),\n",
              " ('became', 'cooler', 'drier'),\n",
              " ('cameroon', 'gabon', 'mozambique'),\n",
              " ('carthaginian', 'general', 'hannibal'),\n",
              " ('climate', 'became', 'cooler'),\n",
              " ('concentration', 'silica', 'abrasive'),\n",
              " ('cooler', 'drier', 'pliocene'),\n",
              " ('distantly', 'related', 'sea'),\n",
              " ('early', 'ancestors', 'palaeocene'),\n",
              " ('eocene', 'small', 'semiaquatic'),\n",
              " ('exists', 'outside', 'protected'),\n",
              " ('feeders', 'generalist', 'eaters'),\n",
              " ('forests', 'extended', 'grassland'),\n",
              " ('general', 'hannibal', 'took'),\n",
              " ('heavy', 'work', 'like'),\n",
              " ('high', 'concentration', 'silica'),\n",
              " ('ii', 'status', 'allows'),\n",
              " ('includes', 'mammoth', 'mastodon'),\n",
              " ('little', 'wear', 'indicating')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26oc4E4fKghi"
      },
      "source": [
        "## Weighted Tag Based Extraction\n",
        "\n",
        "\n",
        "We will now look at a slightly different approach to extracting keyphrases. This method borrows concepts from a couple of papers, namely K. Barker and N. Cornachhia. \"Using Noun Phrase Heads to Extract Document Keyphrases\" and Ian Witten et al. \"KEA: Practical Automatic Keyphrase Extraction\" which you can refer to if you are more interested in further details on their experimentations and approaches. \n",
        "\n",
        "\n",
        "We follow a two-step process in our algorithm here. These steps are mentioned as follows.\n",
        "\n",
        "- Extract all noun phrase chunks using shallow parsing\n",
        "- Compute TF-IDF weights for each chunk and return the top weighted phrases\n",
        "\n",
        "\n",
        "For the first step, we will use a simple pattern based on parts of speech (POS) tags to extract noun phrase chunks. \n",
        "\n",
        "Chunking is a process of extracting phrases from unstructured text, which means analyzing a sentence to identify the constituents(Noun Groups, Verbs, verb groups, etc.) However, it does not specify their internal structure, nor their role in the main sentence. It works on top of POS tagging.\n",
        "\n",
        "We use our sample description of elephants taken from Wikipedia.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2uOISMMKigd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aeee7ab7-80a5-4859-be14-ebda7cb19ec4"
      },
      "source": [
        "elephants[:10]"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Elephants are the largest living land mammals.',\n",
              " 'The largest elephant recorded was one shot in Angola, 1974.',\n",
              " 'It weighed 27,060 pounds (13.5 tons) and stood 13 feet 8 inches tall.',\n",
              " 'Their skin color is grey.',\n",
              " 'At birth, an elephant calf may weigh as much as 100 kg (225 pounds).',\n",
              " 'The baby elephant develops for 20 to 22 months inside its mother.',\n",
              " 'No other land animal takes this long to develop before being born.',\n",
              " 'In the wild, elephants have strong family relationship.',\n",
              " 'Their ways of acting toward other elephants are hard for people to understand.',\n",
              " 'They \"talk\" to each other with very low sounds.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZZ7O9fSp3PS"
      },
      "source": [
        "### Simple Text Pre-processor\r\n",
        "\r\n",
        "We use this just to remove unnecessary special characters and extra whitespaces"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysPqO67jKlKY"
      },
      "source": [
        "def normalize_document_simple(doc):\r\n",
        "    # lower case and remove special characters\\whitespaces\r\n",
        "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, flags=re.I|re.A)\r\n",
        "    doc = doc.strip()\r\n",
        "    # tokenize document\r\n",
        "    tokens = nltk.word_tokenize(doc)\r\n",
        "    # re-create document from whitespace stripped tokens\r\n",
        "    doc = ' '.join([token.strip() for token in tokens])\r\n",
        "    return doc"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeH-tj9cKlDk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bca40fef-9422-49e2-bf0b-a24f2331a6ed"
      },
      "source": [
        "norm_elephants_simple = list(filter(None, [normalize_document_simple(line)\n",
        "                                  for line in elephants]))\n",
        "norm_elephants_simple[:10]"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Elephants are the largest living land mammals',\n",
              " 'The largest elephant recorded was one shot in Angola',\n",
              " 'It weighed pounds tons and stood feet inches tall',\n",
              " 'Their skin color is grey',\n",
              " 'At birth an elephant calf may weigh as much as kg pounds',\n",
              " 'The baby elephant develops for to months inside its mother',\n",
              " 'No other land animal takes this long to develop before being born',\n",
              " 'In the wild elephants have strong family relationship',\n",
              " 'Their ways of acting toward other elephants are hard for people to understand',\n",
              " 'They talk to each other with very low sounds']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDytvQhIqCNy"
      },
      "source": [
        "Now that we have our corpus ready, we will use the pattern, \r\n",
        "\r\n",
        "__`\" NP: {<DT>? <JJ>* <NN.*>+}\"`__ \r\n",
        "\r\n",
        "for extracting all possible noun phrases from our corpus of documents\\sentences. You can always experiment with more sophisticated patterns later incorporating verb, adjective or even adverb phrases. \r\n",
        "\r\n",
        "However we keep things simple and concise here to focus on the core logic. Once we have our pattern, we will define a function to parse and extract these phrases "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGdjPFv3Kkob"
      },
      "source": [
        "import itertools\n",
        "stopwords = nltk.corpus.stopwords.words('english')"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yiw5ZEyzKt21"
      },
      "source": [
        "def get_chunks(sentences, grammar = r'NP: {<DT>? <JJ>* <NN.*>+}', stopword_list=stopwords):\n",
        "    \n",
        "    all_chunks = []\n",
        "    chunker = nltk.chunk.regexp.RegexpParser(grammar)\n",
        "    \n",
        "    for sentence in sentences:\n",
        "        \n",
        "        tagged_sents = [nltk.pos_tag(nltk.word_tokenize(sentence))]   \n",
        "        \n",
        "        chunks = [chunker.parse(tagged_sent) \n",
        "                      for tagged_sent in tagged_sents]\n",
        "        \n",
        "        wtc_sents = [nltk.chunk.tree2conlltags(chunk)\n",
        "                         for chunk in chunks]    \n",
        "        \n",
        "        flattened_chunks = list(\n",
        "                            itertools.chain.from_iterable(\n",
        "                                wtc_sent for wtc_sent in wtc_sents)\n",
        "                           )\n",
        "        \n",
        "        valid_chunks_tagged = [(status, [wtc for wtc in chunk]) \n",
        "                                   for status, chunk \n",
        "                                       in itertools.groupby(flattened_chunks, \n",
        "                                                lambda word_pos_chunk: word_pos_chunk[2] != 'O')]\n",
        "        \n",
        "        valid_chunks = list(set([' '.join(word.lower() \n",
        "                                for word, tag, chunk in wtc_group \n",
        "                                    if word.lower() not in stopword_list) \n",
        "                                        for status, wtc_group in valid_chunks_tagged\n",
        "                                            if status]))\n",
        "        \n",
        "        if valid_chunks not in all_chunks and valid_chunks:\n",
        "          all_chunks.append(valid_chunks)\n",
        "    \n",
        "    return all_chunks"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39hmSoUGqSJG"
      },
      "source": [
        "In the above function we have a defined grammar pattern for chunking or extracting noun phrases. \r\n",
        "\r\n",
        "- We define a chunker over the same pattern and for each sentence in the document\r\n",
        "- We first annotate it with its POS tags and then build a shallow parse tree with noun phrases as the chunks and all other POS tag based words as chinks which are not parts of any chunks\r\n",
        "- Once this is done, we use the tree2conlltags function to generate (w,t,c) triples which are words, POS tags and the IOB formatted chunk tags \r\n",
        "- We remove all tags with chunk tag of 'O' since they are basically words or terms which do not belong to any chunk \r\n",
        "- Finally from these valid chunks, we combine the chunked terms to generate phrases from each chunk group\r\n",
        "\r\n",
        "_Refer to Text Analytics with Python Chapter 3 to dive into shallow parsing and chunking if needed_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ezulor4ZKtuQ"
      },
      "source": [
        "chunks = get_chunks(norm_elephants_simple)"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpWJ2hbkfupL",
        "outputId": "0ec3ee6c-fc59-4b20-d73d-6f2c7607ea30"
      },
      "source": [
        "chunks[:50]"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['living land mammals', 'elephants'],\n",
              " ['angola', 'elephant', 'shot'],\n",
              " ['feet inches', 'pounds tons'],\n",
              " ['skin color'],\n",
              " ['elephant calf', 'kg pounds'],\n",
              " ['mother', 'months', 'baby'],\n",
              " ['land'],\n",
              " ['wild elephants', 'strong family relationship'],\n",
              " ['elephants', 'people', 'ways'],\n",
              " ['low sounds'],\n",
              " ['elephants sounds', 'low people'],\n",
              " ['sounds', 'elephants'],\n",
              " ['strong leathery skin', 'elephants'],\n",
              " ['elephants', 'genera'],\n",
              " ['asian elephants elephas maximus', 'african loxodonta africanus'],\n",
              " ['obvious part', 'trunk'],\n",
              " ['upper lip', 'trunk'],\n",
              " ['food', 'objects', 'trunk'],\n",
              " ['rest', 'trunk', 'elephants hide'],\n",
              " ['elephants trunk', 'elephants', 'symbiotic ants', 'inside', 'acacia trees'],\n",
              " ['tusks', 'elephants'],\n",
              " ['tusks', 'upper jaws', 'large teeth'],\n",
              " ['elephant tusks', 'ivory', 'lot'],\n",
              " ['ivory traders', 'many elephants'],\n",
              " ['trunk'],\n",
              " ['blows', 'trunk'],\n",
              " ['signal', 'elephants', 'wildlife'],\n",
              " ['african elephants', 'ears'],\n",
              " ['leaves branches', 'grass', 'lot', 'grazers'],\n",
              " ['body', 'big ears', 'blood', 'many veins'],\n",
              " ['african elephants', 'ears', 'biologists', 'blood'],\n",
              " ['elephants', 'africa', 'weather', 'asia'],\n",
              " ['tusks', 'female african elephants', 'female asian elephants'],\n",
              " ['african elephants', 'back', 'low place'],\n",
              " ['african elephants', 'end', 'asian elephants', 'trunks', 'fingers'],\n",
              " ['indian elephants', 'grass'],\n",
              " ['high concentration', 'teeth', 'silica', 'grass'],\n",
              " ['teeth', 'elephants', 'sequence'],\n",
              " ['tooth', 'time', 'jaw total'],\n",
              " ['front teeth', 'premolars', 'molars', 'teeth'],\n",
              " ['last molar wears', 'elephant'],\n",
              " ['years'],\n",
              " ['elephants', 'circus people', 'soft food', 'zoo'],\n",
              " ['savanna', 'african elephants', 'others', 'forest'],\n",
              " ['different species', 'today many people'],\n",
              " ['savanna group loxodonta africanus',\n",
              "  'scientists',\n",
              "  'group loxodonta cyclotis'],\n",
              " ['elephants', 'large aquatic mammals', 'sea cows'],\n",
              " ['small semiaquatic animals', 'palaeocene', 'eocene', 'early ancestors'],\n",
              " ['miocene several groups', 'gomphotheres', 'elephants', 'deinotheres'],\n",
              " ['teeth', 'vegetation', 'diet']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAsuG6HKjwBl"
      },
      "source": [
        "### TF-IDF weighing of Chunks\r\n",
        "\r\n",
        "We will now build on top of our `get_chunks()` function by implementing the necessary logic for Step 2 where we will build a TF-IDF based model on our keyphrases using `gensim` and then compute TF-IDF based weights for each keyphrase based on its occurrence in the corpus. \r\n",
        "\r\n",
        "Finally we will sort these keyphrases based on their TF-IDF weights and show the top N keyphrases where `top_n` is specified by the user."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ka9Rdq-VK0dS"
      },
      "source": [
        "from gensim import corpora, models\n",
        "\n",
        "def get_tfidf_weighted_keyphrases(sentences, \n",
        "                                  grammar=r'NP: {<DT>? <JJ>* <NN.*>+}',\n",
        "                                  top_n=10):\n",
        "    \n",
        "    valid_chunks = get_chunks(sentences, grammar=grammar)\n",
        "                                     \n",
        "    dictionary = corpora.Dictionary(valid_chunks)\n",
        "    corpus = [dictionary.doc2bow(chunk) for chunk in valid_chunks]\n",
        "    \n",
        "    tfidf = models.TfidfModel(corpus)\n",
        "    corpus_tfidf = tfidf[corpus]\n",
        "    \n",
        "    weighted_phrases = {dictionary.get(idx): value \n",
        "                           for doc in corpus_tfidf \n",
        "                               for idx, value in doc}\n",
        "                            \n",
        "    weighted_phrases = sorted(weighted_phrases.items(), \n",
        "                              key=itemgetter(1), reverse=True)\n",
        "    weighted_phrases = [(term, round(wt, 3)) for term, wt in weighted_phrases]\n",
        "    \n",
        "    return weighted_phrases[:top_n]"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJ1eU37YK0Qe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca4efeb1-ff11-4699-f36f-aeba4f43f434"
      },
      "source": [
        "get_tfidf_weighted_keyphrases(sentences=norm_elephants_simple, top_n=50)"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('skin color', 1.0),\n",
              " ('land', 1.0),\n",
              " ('low sounds', 1.0),\n",
              " ('many circuses', 1.0),\n",
              " ('living land mammals', 0.956),\n",
              " ('sounds', 0.956),\n",
              " ('strong leathery skin', 0.956),\n",
              " ('genera', 0.956),\n",
              " ('tourists', 0.94),\n",
              " ('obvious part', 0.866),\n",
              " ('upper lip', 0.866),\n",
              " ('blows', 0.866),\n",
              " ('last molar wears', 0.796),\n",
              " ('gomphotheres teeth', 0.796),\n",
              " ('largescale', 0.796),\n",
              " ('ways', 0.795),\n",
              " ('sequence', 0.795),\n",
              " ('war', 0.795),\n",
              " ('criminals', 0.762),\n",
              " ('range exists', 0.762),\n",
              " ('elephants gestation', 0.762),\n",
              " ('rides', 0.742),\n",
              " ('conservation efforts', 0.742),\n",
              " ('products ivory meat', 0.742),\n",
              " ('feet inches', 0.707),\n",
              " ('pounds tons', 0.707),\n",
              " ('elephant calf', 0.707),\n",
              " ('kg pounds', 0.707),\n",
              " ('strong family relationship', 0.707),\n",
              " ('wild elephants', 0.707),\n",
              " ('elephants sounds', 0.707),\n",
              " ('low people', 0.707),\n",
              " ('african loxodonta africanus', 0.707),\n",
              " ('asian elephants elephas maximus', 0.707),\n",
              " ('ivory traders', 0.707),\n",
              " ('many elephants', 0.707),\n",
              " ('different species', 0.707),\n",
              " ('today many people', 0.707),\n",
              " ('favoured specialist grass feeders', 0.707),\n",
              " ('generalist eaters', 0.707),\n",
              " ('heat', 0.707),\n",
              " ('male elephant', 0.707),\n",
              " ('musth', 0.707),\n",
              " ('state', 0.707),\n",
              " ('half hour', 0.707),\n",
              " ('newborn elephant', 0.707),\n",
              " ('liters', 0.707),\n",
              " ('milk every day', 0.707),\n",
              " ('fun', 0.696),\n",
              " ('signal', 0.691)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2_OvxW_q6wT"
      },
      "source": [
        "We can also leverage gensim's `summarization` module which has a keywords function which can be used to extract keywords or phrases from text. \r\n",
        "\r\n",
        "This uses a variation of the TextRank algorithm which we shall be exploring during document summarization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOl73ZMqK0Jn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "840a26b3-2019-457f-fdfe-4e2a8cf795a1"
      },
      "source": [
        "from gensim.summarization import keywords\n",
        "\n",
        "key_words = keywords(' '.join(elephants), ratio=1.0, scores=True, lemmatize=True)\n",
        "[(item, round(score, 3)) for item, score in key_words][:50]"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('elephant recorded', 0.339),\n",
              " ('african', 0.127),\n",
              " ('called', 0.094),\n",
              " ('babies', 0.091),\n",
              " ('female', 0.09),\n",
              " ('people', 0.088),\n",
              " ('lives', 0.085),\n",
              " ('species', 0.084),\n",
              " ('grass', 0.082),\n",
              " ('south', 0.078),\n",
              " ('legal', 0.076),\n",
              " ('humans', 0.075),\n",
              " ('loxodonta', 0.074),\n",
              " ('ears', 0.073),\n",
              " ('animals', 0.072),\n",
              " ('aquatic', 0.072),\n",
              " ('reduced forests', 0.071),\n",
              " ('long', 0.07),\n",
              " ('estimate', 0.07),\n",
              " ('pounds', 0.07),\n",
              " ('upper', 0.07),\n",
              " ('strong family', 0.07),\n",
              " ('different', 0.068),\n",
              " ('large teeth', 0.068),\n",
              " ('mother', 0.064),\n",
              " ('eat', 0.064),\n",
              " ('inches', 0.062),\n",
              " ('gomphotheres', 0.062),\n",
              " ('feet', 0.062),\n",
              " ('ivory comes', 0.062),\n",
              " ('mainly', 0.061),\n",
              " ('namibia', 0.058),\n",
              " ('groups', 0.057),\n",
              " ('botswana', 0.057),\n",
              " ('mammals', 0.057),\n",
              " ('modern', 0.057),\n",
              " ('low', 0.057),\n",
              " ('indians', 0.057),\n",
              " ('sounds', 0.054),\n",
              " ('countries sport', 0.054),\n",
              " ('allows restricted', 0.054),\n",
              " ('carry blood', 0.054),\n",
              " ('land', 0.053),\n",
              " ('asians', 0.053),\n",
              " ('remaining population probably succumbed', 0.052),\n",
              " ('tanzania', 0.051),\n",
              " ('leathery', 0.051),\n",
              " ('things', 0.051),\n",
              " ('biologists think', 0.051),\n",
              " ('soft', 0.049)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDWy4XZVLFDw"
      },
      "source": [
        "## Bonus: Extraction using RAKE\n",
        "\n",
        "RAKE short for Rapid Automatic Keyword Extraction algorithm, is a domain independent keyword extraction algorithm which tries to determine key phrases in a body of text by analyzing the frequency of word appearance and its co-occurance with other words in the text.\n",
        "\n",
        "Full paper is available here: [researchgate](https://www.researchgate.net/profile/Stuart_Rose/publication/227988510_Automatic_Keyword_Extraction_from_Individual_Documents/links/55071c570cf27e990e04c8bb.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Y9FWkhYLHx2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb02fe71-6f74-4b53-dd64-5dba376a9cb3"
      },
      "source": [
        "!pip install rake-nltk"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting rake-nltk\n",
            "  Downloading https://files.pythonhosted.org/packages/8e/c4/b4ff57e541ac5624ad4b20b89c2bafd4e98f29fd83139f3a81858bdb3815/rake_nltk-1.0.4.tar.gz\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from rake-nltk) (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->rake-nltk) (1.15.0)\n",
            "Building wheels for collected packages: rake-nltk\n",
            "  Building wheel for rake-nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rake-nltk: filename=rake_nltk-1.0.4-py2.py3-none-any.whl size=7819 sha256=e43dc0866028b602cc81ee0818194c746a29a7823625591db618173480431176\n",
            "  Stored in directory: /root/.cache/pip/wheels/ef/92/fc/271b3709e71a96ffe934b27818946b795ac6b9b8ff8682483f\n",
            "Successfully built rake-nltk\n",
            "Installing collected packages: rake-nltk\n",
            "Successfully installed rake-nltk-1.0.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVG1NeUMkhPL"
      },
      "source": [
        "from rake_nltk import Rake"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gN7Pw6GlLd7q"
      },
      "source": [
        "r = Rake()"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTGwr2znLdvX"
      },
      "source": [
        "# Extraction given the text.\n",
        "r.extract_keywords_from_text(' '.join(elephants))"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKtexfP1Lr4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f423837-6dd3-4a64-88da-8723f571fad7"
      },
      "source": [
        "# To get keyword phrases ranked highest to lowest with scores.\n",
        "r.get_ranked_phrases_with_scores()[:50]"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(31.0, 'stood 13 feet 8 inches tall'),\n",
              " (24.5, 'heavy work like lifting trees'),\n",
              " (23.0, 'range exists outside protected areas'),\n",
              " (21.666666666666668, 'actual family elephantidae – evolved'),\n",
              " (19.476190476190474, 'another female elephant often stays'),\n",
              " (18.328735632183907, 'ivory traders killed many elephants'),\n",
              " (16.078735632183907, 'indian elephants eat mainly grass'),\n",
              " (16.0, 'carthaginian general hannibal took'),\n",
              " (15.362068965517242, 'elephants avoid acacia trees'),\n",
              " (14.8, 'favoured specialist grass feeders'),\n",
              " (14.666666666666666, 'teeth show little wear'),\n",
              " (14.166666666666666, 'forest group loxodonta cyclotis'),\n",
              " (14.0, 'largest living land mammals'),\n",
              " (13.976190476190476, 'elephant calf may weigh'),\n",
              " (13.862068965517242, 'asian elephants elephas maximus'),\n",
              " (13.666666666666666, 'savanna group loxodonta africanus'),\n",
              " (13.6, 'weigh around 120 kg'),\n",
              " (13.5, 'calf \") every four'),\n",
              " (13.333333333333334, 'remaining population probably succumbed'),\n",
              " (13.333333333333334, 'forced gomphotheres toward extinction'),\n",
              " (13.142857142857142, 'elephant usually stands still'),\n",
              " (12.278735632183908, 'ears helps african elephants'),\n",
              " (10.0, 'often stand within'),\n",
              " (9.916666666666666, 'african loxodonta africanus'),\n",
              " (9.862068965517242, 'dwelling elephants evolved'),\n",
              " (9.528735632183908, 'especially indian elephants'),\n",
              " (9.333333333333334, 'western ghats may'),\n",
              " (9.142857142857142, 'largest elephant recorded'),\n",
              " (9.0, 'strong family relationship'),\n",
              " (9.0, 'rough estimate ).'),\n",
              " (9.0, 'large mainly forest'),\n",
              " (9.0, 'climate became cooler'),\n",
              " (9.0, 'appendix ii status'),\n",
              " (9.0, '260 lb ).'),\n",
              " (8.862068965517242, 'modern elephants –'),\n",
              " (8.862068965517242, 'keep elephants alive'),\n",
              " (8.862068965517242, 'although asian elephants'),\n",
              " (8.833333333333334, 'two living genera'),\n",
              " (8.833333333333334, 'local numbers may'),\n",
              " (8.695402298850576, 'female asian elephants'),\n",
              " (8.695402298850574, 'female elephants run'),\n",
              " (8.67159277504105, 'used indian elephants'),\n",
              " (8.666666666666666, '12 front teeth'),\n",
              " (8.666666666666666, '000 – 50'),\n",
              " (8.612068965517242, 'african elephants receive'),\n",
              " (8.5, 'miocene several groups'),\n",
              " (8.5, 'last molar wears'),\n",
              " (8.5, 'large aquatic mammals'),\n",
              " (8.5, 'land animal takes'),\n",
              " (8.5, 'carry blood throughout')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    }
  ]
}