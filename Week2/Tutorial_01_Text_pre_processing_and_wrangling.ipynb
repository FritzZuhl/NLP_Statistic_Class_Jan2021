{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tutorial 01 - Text pre-processing and wrangling.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1l-B3avlyc8c"
      },
      "source": [
        "# Tutorial 1: Text Pre-processing and Wrangling\n",
        "\n",
        "Text is different than usual datasets we use to build our typical Machine Learning models. Text data needs to be pre-processed to ensure we have it in a form that is usable for various NLP tasks. Text processing and wrangling is a necessary and important step in any NLP project.\n",
        "\n",
        "In this notebook, we will cover:\n",
        "- Sentence tokenization\n",
        "- Word tokenization\n",
        "- Handling non-text characters - accents, special symbols, HTML\n",
        "- Preprocessing steps such as spell correction, stemming, lemmatization, expanding contractions and stopword removal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIaP2ot7zBGs"
      },
      "source": [
        "### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEHEVTydzBsu"
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "import requests\n",
        "import numpy as np\n",
        "from pprint import pprint\n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fppalTkNGQOW",
        "outputId": "22a18356-5f08-432e-a2c8-6e87077edf1a"
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('europarl_raw')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package europarl_raw to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgkdLpoezF0M"
      },
      "source": [
        "## Get Text Dataset\n",
        "\n",
        "Project Gutenberg is a large opensource and free collection of literary works from across the world. In this case, we will leverage content of the book **The Bible - Book 1: Genesis** for understanding different text processing and wrangling steps in the following sections\n",
        "\n",
        "\n",
        "We will also use a smaller sample text with a few short sentences to demostrate examples as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGdDEGiQzGQ3"
      },
      "source": [
        "bible_html_url = \"http://www.gutenberg.org/cache/epub/8001/pg8001.html\"\n",
        "bible_txt_url = \"http://www.gutenberg.org/cache/epub/8001/pg8001.txt\"\n",
        "data = requests.get(bible_txt_url)\n",
        "content = data.text"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqG5n0kWHpkV",
        "outputId": "6b18baa5-023f-43a2-ff32-91b87a282984"
      },
      "source": [
        "print(content[970:1800])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "Title: The Bible, King James version, Book 1: Genesis\r\n",
            "\r\n",
            "Release Date: May, 2005  [EBook #8001]\r\n",
            "[Yes, we are more than one year ahead of schedule]\r\n",
            "[This file was first posted on June 7, 2003]\r\n",
            "\r\n",
            "\r\n",
            "Edition: 10\r\n",
            "\r\n",
            "Language: English\r\n",
            "\r\n",
            "\r\n",
            "*** START OF THE PROJECT GUTENBERG EBOOK, THE BIBLE, KING JAMES, BOOK 1***\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "This eBook was produced by David Widger\r\n",
            "with the help of Derek Andrew's text from January 1992\r\n",
            "and the work of Bryan Taylor in November 2002.\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "Book 01        Genesis\r\n",
            "\r\n",
            "01:001:001 In the beginning God created the heaven and the earth.\r\n",
            "\r\n",
            "01:001:002 And the earth was without form, and void; and darkness was\r\n",
            "           upon the face of the deep. And the Spirit of God moved upon\r\n",
            "           the face of the waters.\r\n",
            "\r\n",
            "01:001:003 And God said, Let there be light: and there was light.\r\n",
            "\r\n",
            "01\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "McHaUzY-Ga3Y",
        "outputId": "f632808b-be65-4558-bad0-86d327e46e5a"
      },
      "source": [
        "# Total characters in War and Peace\n",
        "len(content)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "262238"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "pyQGCLZfG7l1",
        "outputId": "6ab93941-9c82-4fb6-d6b9-f9af488c4041"
      },
      "source": [
        "sample_text = (\"US unveils world's most powerful supercomputer, beats China. \" \n",
        "               \"The US has unveiled the world's most powerful supercomputer called 'Summit', \" \n",
        "               \"beating the previous record-holder China's Sunway TaihuLight. With a peak performance \"\n",
        "               \"of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, \"\n",
        "               \"which is capable of 93,000 trillion calculations per second. Summit has 4,608 servers, \"\n",
        "               \"which reportedly take up the size of two tennis courts.\")\n",
        "sample_text"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"US unveils world's most powerful supercomputer, beats China. The US has unveiled the world's most powerful supercomputer called 'Summit', beating the previous record-holder China's Sunway TaihuLight. With a peak performance of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, which is capable of 93,000 trillion calculations per second. Summit has 4,608 servers, which reportedly take up the size of two tennis courts.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "FMdH0H78GavY",
        "outputId": "f7bed759-431b-41ad-cde4-2193fa9d2be5"
      },
      "source": [
        "# First 100 characters in the corpus\n",
        "content[0:100]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\ufeffProject Gutenberg EBook The Bible, King James, Book 1: Genesis\\r\\n\\r\\nCopyright laws are changing all o'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OiKe5_Ry29q"
      },
      "source": [
        "## Sentence Tokenization\n",
        "\n",
        "Sentence is a syntactical as well as a logical division of content in a given corpus. In order to understand text or prepare it for various tasks, understanding sentence boundaries is an important step.\n",
        "\n",
        "Sentence tokenization is the process of determining sentence boundaries in a given corpus.\n",
        "\n",
        "One might think that it is merely trivial to determine a sentence boundary, we just need to split based on \".\". While this generally holds true yet there are exceptions. Think of scenarios where we use \".\" in abbreviations and shorthand notations(such as Mr. or Mrs.).\n",
        "\n",
        "\n",
        "Let us now go through a few standard ways of performing sentence tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJDQdvp2y6Ar"
      },
      "source": [
        "### NLTK's Default Tokenizer\n",
        "\n",
        "NLTK provides a number of sentence tokenizers. Let's first have a look at the default one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXaTV42jy6ac",
        "outputId": "7034ce0f-943c-4f62-f634-6fe69f5e7704"
      },
      "source": [
        "default_st = nltk.sent_tokenize\n",
        "bib_sentences = default_st(text=content)\n",
        "sample_sentences = default_st(text=sample_text)\n",
        "\n",
        "print('Total sentences in sample_text:{}'.format(len(sample_sentences)))\n",
        "print('Sample text sentences :-')\n",
        "print(np.array(sample_sentences))\n",
        "\n",
        "print('\\nTotal sentences in bible:', len(bib_sentences))\n",
        "print('First 5 sentences in bible:-')\n",
        "print(np.array(bib_sentences[0:5]))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total sentences in sample_text:4\n",
            "Sample text sentences :-\n",
            "[\"US unveils world's most powerful supercomputer, beats China.\"\n",
            " \"The US has unveiled the world's most powerful supercomputer called 'Summit', beating the previous record-holder China's Sunway TaihuLight.\"\n",
            " 'With a peak performance of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, which is capable of 93,000 trillion calculations per second.'\n",
            " 'Summit has 4,608 servers, which reportedly take up the size of two tennis courts.']\n",
            "\n",
            "Total sentences in bible: 1560\n",
            "First 5 sentences in bible:-\n",
            "['\\ufeffProject Gutenberg EBook The Bible, King James, Book 1: Genesis\\r\\n\\r\\nCopyright laws are changing all over the world.'\n",
            " 'Be sure to check the\\r\\ncopyright laws for your country before downloading or redistributing\\r\\nthis or any other Project Gutenberg eBook.'\n",
            " 'This header should be the first thing seen when viewing this Project\\r\\nGutenberg file.'\n",
            " 'Please do not remove it.'\n",
            " 'Do not change or edit the\\r\\nheader without written permission.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKtN10SOzupR"
      },
      "source": [
        "### Punkt Tokenizer\n",
        "\n",
        "This tokenizer divides a text into a list of sentences by using an unsupervised algorithm to build a model for abbreviation words, collocations, and words that start sentences. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sr5RElGfIfcS",
        "outputId": "7f9997d2-59d0-4586-93d8-feb3b4e9f4ec"
      },
      "source": [
        "punkt_st = nltk.tokenize.PunktSentenceTokenizer()\n",
        "sample_sentences = punkt_st.tokenize(sample_text)\n",
        "print(np.array(sample_sentences))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\"US unveils world's most powerful supercomputer, beats China.\"\n",
            " \"The US has unveiled the world's most powerful supercomputer called 'Summit', beating the previous record-holder China's Sunway TaihuLight.\"\n",
            " 'With a peak performance of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, which is capable of 93,000 trillion calculations per second.'\n",
            " 'Summit has 4,608 servers, which reportedly take up the size of two tennis courts.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-YF0Eufz0vx"
      },
      "source": [
        "### Regex Tokenizer\n",
        "\n",
        "A RegexpTokenizer splits a string into substrings using predefined patterns (needs to be defined by the user)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wscQa-vYIfSj",
        "outputId": "b8ce8ab9-2291-49f9-f6d1-f51b60e989f2"
      },
      "source": [
        "SENTENCE_TOKENS_PATTERN = r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<![A-Z]\\.)(?<=\\.|\\?|\\!)\\s'\n",
        "regex_st = nltk.tokenize.RegexpTokenizer(\n",
        "            pattern=SENTENCE_TOKENS_PATTERN,\n",
        "            gaps=True)\n",
        "sample_sentences = regex_st.tokenize(sample_text)\n",
        "print(np.array(sample_sentences))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\"US unveils world's most powerful supercomputer, beats China.\"\n",
            " \"The US has unveiled the world's most powerful supercomputer called 'Summit', beating the previous record-holder China's Sunway TaihuLight.\"\n",
            " 'With a peak performance of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, which is capable of 93,000 trillion calculations per second.'\n",
            " 'Summit has 4,608 servers, which reportedly take up the size of two tennis courts.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5CkXCmoy8p2"
      },
      "source": [
        "### Tokenize German Sentences\n",
        "\n",
        "NLTK also provides utilities to handle sentence tokenization for various languages (apart from English). The following is a sample to showcase sentence tokenization for German text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IP3Ruqpty8-0",
        "outputId": "177a8a5e-1c78-412b-c5c1-7c9dad055123"
      },
      "source": [
        "from nltk.corpus import europarl_raw\n",
        "\n",
        "german_text = europarl_raw.german.raw(fileids='ep-00-01-17.de')\n",
        "# Total characters in the corpus\n",
        "print(len(german_text))\n",
        "# First 100 characters in the corpus\n",
        "print(german_text[0:100])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "157171\n",
            " \n",
            "Wiederaufnahme der Sitzungsperiode Ich erkläre die am Freitag , dem 17. Dezember unterbrochene Sit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxEr07ZYIDiv"
      },
      "source": [
        "# default sentence tokenizer \n",
        "german_sentences_def = default_st(text=german_text, language='german')\n",
        "\n",
        "# loading german text tokenizer into a PunktSentenceTokenizer instance  \n",
        "german_tokenizer = nltk.data.load(resource_url='tokenizers/punkt/german.pickle')\n",
        "german_sentences = german_tokenizer.tokenize(german_text)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HatWlFiCIDaC",
        "outputId": "eaa26e73-6b21-48fe-cdef-d2df815f5a0a"
      },
      "source": [
        "# check if results of both tokenizers match \n",
        "# should be True\n",
        "print(german_sentences_def == german_sentences)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmqmLhjZIP7R",
        "outputId": "ec3b5c5d-a247-4f64-b8d6-adcf3eaf3538"
      },
      "source": [
        "# print first 5 sentences of the corpus\n",
        "print(np.array(german_sentences[:5]))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[' \\nWiederaufnahme der Sitzungsperiode Ich erkläre die am Freitag , dem 17. Dezember unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen , wünsche Ihnen nochmals alles Gute zum Jahreswechsel und hoffe , daß Sie schöne Ferien hatten .'\n",
            " 'Wie Sie feststellen konnten , ist der gefürchtete \" Millenium-Bug \" nicht eingetreten .'\n",
            " 'Doch sind Bürger einiger unserer Mitgliedstaaten Opfer von schrecklichen Naturkatastrophen geworden .'\n",
            " 'Im Parlament besteht der Wunsch nach einer Aussprache im Verlauf dieser Sitzungsperiode in den nächsten Tagen .'\n",
            " 'Heute möchte ich Sie bitten - das ist auch der Wunsch einiger Kolleginnen und Kollegen - , allen Opfern der Stürme , insbesondere in den verschiedenen Ländern der Europäischen Union , in einer Schweigeminute zu gedenken .']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iT5vSgSz8C3"
      },
      "source": [
        "## Word Tokenization\n",
        "\n",
        "Word can be considered as a basic building block for NLP tasks. A combination of words make up a sentence. As in the previous section, we worked towards understanding sentence tokenization, in this section, we will focus on understanding word boundaries.\n",
        "\n",
        "We will make use of various word tokenizers available from ``nltk`` as well as ``spacy`` in this section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VoL-Yo18z932",
        "outputId": "32d35496-db07-4822-ad70-b938138093fd"
      },
      "source": [
        "# default tokenizer\n",
        "default_wt = nltk.word_tokenize\n",
        "words = default_wt(sample_text)\n",
        "np.array(words)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['US', 'unveils', 'world', \"'s\", 'most', 'powerful',\n",
              "       'supercomputer', ',', 'beats', 'China', '.', 'The', 'US', 'has',\n",
              "       'unveiled', 'the', 'world', \"'s\", 'most', 'powerful',\n",
              "       'supercomputer', 'called', \"'Summit\", \"'\", ',', 'beating', 'the',\n",
              "       'previous', 'record-holder', 'China', \"'s\", 'Sunway', 'TaihuLight',\n",
              "       '.', 'With', 'a', 'peak', 'performance', 'of', '200,000',\n",
              "       'trillion', 'calculations', 'per', 'second', ',', 'it', 'is',\n",
              "       'over', 'twice', 'as', 'fast', 'as', 'Sunway', 'TaihuLight', ',',\n",
              "       'which', 'is', 'capable', 'of', '93,000', 'trillion',\n",
              "       'calculations', 'per', 'second', '.', 'Summit', 'has', '4,608',\n",
              "       'servers', ',', 'which', 'reportedly', 'take', 'up', 'the', 'size',\n",
              "       'of', 'two', 'tennis', 'courts', '.'], dtype='<U13')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_AXuLgt0EgS"
      },
      "source": [
        "### Treebank Tokenizer\n",
        "\n",
        "The Treebank tokenizer uses regular expressions to tokenize text as in [Penn Treebank](https://web.archive.org/web/19970614072242if_/http://www.cis.upenn.edu/~treebank/tokenization.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njditD1q0CDu",
        "outputId": "f0b2dcf2-33ff-48f5-8236-ac2515d1526f"
      },
      "source": [
        "treebank_wt = nltk.TreebankWordTokenizer()\n",
        "words = treebank_wt.tokenize(sample_text)\n",
        "np.array(words)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['US', 'unveils', 'world', \"'s\", 'most', 'powerful',\n",
              "       'supercomputer', ',', 'beats', 'China.', 'The', 'US', 'has',\n",
              "       'unveiled', 'the', 'world', \"'s\", 'most', 'powerful',\n",
              "       'supercomputer', 'called', \"'Summit\", \"'\", ',', 'beating', 'the',\n",
              "       'previous', 'record-holder', 'China', \"'s\", 'Sunway',\n",
              "       'TaihuLight.', 'With', 'a', 'peak', 'performance', 'of', '200,000',\n",
              "       'trillion', 'calculations', 'per', 'second', ',', 'it', 'is',\n",
              "       'over', 'twice', 'as', 'fast', 'as', 'Sunway', 'TaihuLight', ',',\n",
              "       'which', 'is', 'capable', 'of', '93,000', 'trillion',\n",
              "       'calculations', 'per', 'second.', 'Summit', 'has', '4,608',\n",
              "       'servers', ',', 'which', 'reportedly', 'take', 'up', 'the', 'size',\n",
              "       'of', 'two', 'tennis', 'courts', '.'], dtype='<U13')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YM8DB4XB0IwU"
      },
      "source": [
        "### TokTok Tokenizer\n",
        "\n",
        "The tok-tok tokenizer is a simple, general tokenizer, where the input has one sentence per line; thus only final period is tokenized.\n",
        "\n",
        "Tok-tok has been tested on, and gives reasonably good results for English, Persian, Russian, Czech, French, German, Vietnamese, Tajik, and a few others. The input should be in UTF-8 encoding.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYqMXr4S0JN5",
        "outputId": "e59957a6-165e-458d-a98c-33b9f75575c3"
      },
      "source": [
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "tokenizer = ToktokTokenizer()\n",
        "words = tokenizer.tokenize(sample_text)\n",
        "np.array(words)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['US', 'unveils', 'world', \"'\", 's', 'most', 'powerful',\n",
              "       'supercomputer', ',', 'beats', 'China.', 'The', 'US', 'has',\n",
              "       'unveiled', 'the', 'world', \"'\", 's', 'most', 'powerful',\n",
              "       'supercomputer', 'called', \"'\", 'Summit', \"'\", ',', 'beating',\n",
              "       'the', 'previous', 'record-holder', 'China', \"'\", 's', 'Sunway',\n",
              "       'TaihuLight.', 'With', 'a', 'peak', 'performance', 'of', '200,000',\n",
              "       'trillion', 'calculations', 'per', 'second', ',', 'it', 'is',\n",
              "       'over', 'twice', 'as', 'fast', 'as', 'Sunway', 'TaihuLight', ',',\n",
              "       'which', 'is', 'capable', 'of', '93,000', 'trillion',\n",
              "       'calculations', 'per', 'second.', 'Summit', 'has', '4,608',\n",
              "       'servers', ',', 'which', 'reportedly', 'take', 'up', 'the', 'size',\n",
              "       'of', 'two', 'tennis', 'courts', '.'], dtype='<U13')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbW_yznA0SUy"
      },
      "source": [
        "### Custom Tokenizer\n",
        "\n",
        "``nltk`` also allows us to create our own custom tokenizers using regular expressions, whitespaces, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bi9fUVe0T1D",
        "outputId": "22cae2b5-a9e1-4bd0-8bc2-8e765207aa24"
      },
      "source": [
        "# regex based custom tokenizer-1\n",
        "GAP_PATTERN = r'\\s+'        \n",
        "regex_wt = nltk.RegexpTokenizer(pattern=GAP_PATTERN,\n",
        "                                gaps=True)\n",
        "words = regex_wt.tokenize(sample_text)\n",
        "np.array(words)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['US', 'unveils', \"world's\", 'most', 'powerful', 'supercomputer,',\n",
              "       'beats', 'China.', 'The', 'US', 'has', 'unveiled', 'the',\n",
              "       \"world's\", 'most', 'powerful', 'supercomputer', 'called',\n",
              "       \"'Summit',\", 'beating', 'the', 'previous', 'record-holder',\n",
              "       \"China's\", 'Sunway', 'TaihuLight.', 'With', 'a', 'peak',\n",
              "       'performance', 'of', '200,000', 'trillion', 'calculations', 'per',\n",
              "       'second,', 'it', 'is', 'over', 'twice', 'as', 'fast', 'as',\n",
              "       'Sunway', 'TaihuLight,', 'which', 'is', 'capable', 'of', '93,000',\n",
              "       'trillion', 'calculations', 'per', 'second.', 'Summit', 'has',\n",
              "       '4,608', 'servers,', 'which', 'reportedly', 'take', 'up', 'the',\n",
              "       'size', 'of', 'two', 'tennis', 'courts.'], dtype='<U14')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "siF2yKgWI9Go",
        "outputId": "5dfe3763-4ee9-44b2-8d0d-075b72de75a2"
      },
      "source": [
        "# whitespace tokenizer\n",
        "whitespace_wt = nltk.WhitespaceTokenizer()\n",
        "words = whitespace_wt.tokenize(sample_text)\n",
        "np.array(words)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['US', 'unveils', \"world's\", 'most', 'powerful', 'supercomputer,',\n",
              "       'beats', 'China.', 'The', 'US', 'has', 'unveiled', 'the',\n",
              "       \"world's\", 'most', 'powerful', 'supercomputer', 'called',\n",
              "       \"'Summit',\", 'beating', 'the', 'previous', 'record-holder',\n",
              "       \"China's\", 'Sunway', 'TaihuLight.', 'With', 'a', 'peak',\n",
              "       'performance', 'of', '200,000', 'trillion', 'calculations', 'per',\n",
              "       'second,', 'it', 'is', 'over', 'twice', 'as', 'fast', 'as',\n",
              "       'Sunway', 'TaihuLight,', 'which', 'is', 'capable', 'of', '93,000',\n",
              "       'trillion', 'calculations', 'per', 'second.', 'Summit', 'has',\n",
              "       '4,608', 'servers,', 'which', 'reportedly', 'take', 'up', 'the',\n",
              "       'size', 'of', 'two', 'tennis', 'courts.'], dtype='<U14')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVmk8ZLnJG0E",
        "outputId": "9825dba1-b229-4f77-de8a-ea0501bb89cc"
      },
      "source": [
        "# utility for tokenization\n",
        "def tokenize_text(text):\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    word_tokens = [nltk.word_tokenize(sentence) for sentence in sentences] \n",
        "    return word_tokens\n",
        "\n",
        "sents = tokenize_text(sample_text)\n",
        "np.array(sents)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([list(['US', 'unveils', 'world', \"'s\", 'most', 'powerful', 'supercomputer', ',', 'beats', 'China', '.']),\n",
              "       list(['The', 'US', 'has', 'unveiled', 'the', 'world', \"'s\", 'most', 'powerful', 'supercomputer', 'called', \"'Summit\", \"'\", ',', 'beating', 'the', 'previous', 'record-holder', 'China', \"'s\", 'Sunway', 'TaihuLight', '.']),\n",
              "       list(['With', 'a', 'peak', 'performance', 'of', '200,000', 'trillion', 'calculations', 'per', 'second', ',', 'it', 'is', 'over', 'twice', 'as', 'fast', 'as', 'Sunway', 'TaihuLight', ',', 'which', 'is', 'capable', 'of', '93,000', 'trillion', 'calculations', 'per', 'second', '.']),\n",
              "       list(['Summit', 'has', '4,608', 'servers', ',', 'which', 'reportedly', 'take', 'up', 'the', 'size', 'of', 'two', 'tennis', 'courts', '.'])],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rhi2_I9_JRa7",
        "outputId": "450fb217-25eb-45ea-bec4-aaafdf64055b"
      },
      "source": [
        "words = [word for sentence in sents for word in sentence]\n",
        "np.array(words)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['US', 'unveils', 'world', \"'s\", 'most', 'powerful',\n",
              "       'supercomputer', ',', 'beats', 'China', '.', 'The', 'US', 'has',\n",
              "       'unveiled', 'the', 'world', \"'s\", 'most', 'powerful',\n",
              "       'supercomputer', 'called', \"'Summit\", \"'\", ',', 'beating', 'the',\n",
              "       'previous', 'record-holder', 'China', \"'s\", 'Sunway', 'TaihuLight',\n",
              "       '.', 'With', 'a', 'peak', 'performance', 'of', '200,000',\n",
              "       'trillion', 'calculations', 'per', 'second', ',', 'it', 'is',\n",
              "       'over', 'twice', 'as', 'fast', 'as', 'Sunway', 'TaihuLight', ',',\n",
              "       'which', 'is', 'capable', 'of', '93,000', 'trillion',\n",
              "       'calculations', 'per', 'second', '.', 'Summit', 'has', '4,608',\n",
              "       'servers', ',', 'which', 'reportedly', 'take', 'up', 'the', 'size',\n",
              "       'of', 'two', 'tennis', 'courts', '.'], dtype='<U13')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZmp-xdsKJo3"
      },
      "source": [
        "### Spacy Tokenizer\n",
        "\n",
        "``spacy`` provides easy to use interfaces to perform sentence and word tokenization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pIu_CmNJfq9"
      },
      "source": [
        "import spacy"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-MYBs8LJRUy"
      },
      "source": [
        "nlp = spacy.load('en', parse = True, tag=True, entity=True)\n",
        "\n",
        "text_spacy = nlp(sample_text)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgW9AQ05JRL-",
        "outputId": "d7e681a4-a3df-4787-c402-85c109c3a39e"
      },
      "source": [
        "sents = np.array(list(text_spacy.sents))\n",
        "sents"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([US unveils world's most powerful supercomputer, beats China.,\n",
              "       The US has unveiled the world's most powerful supercomputer called 'Summit', beating the previous record-holder China's Sunway TaihuLight.,\n",
              "       With a peak performance of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, which is capable of 93,000 trillion calculations per second.,\n",
              "       Summit has 4,608 servers, which reportedly take up the size of two tennis courts.],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4tVKi5_KD5S",
        "outputId": "733880bc-4cc6-44cb-e5ca-ac9184f6546c"
      },
      "source": [
        "sent_words = [[word.text for word in sent] for sent in sents]\n",
        "np.array(sent_words)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([list(['US', 'unveils', 'world', \"'s\", 'most', 'powerful', 'supercomputer', ',', 'beats', 'China', '.']),\n",
              "       list(['The', 'US', 'has', 'unveiled', 'the', 'world', \"'s\", 'most', 'powerful', 'supercomputer', 'called', \"'\", 'Summit', \"'\", ',', 'beating', 'the', 'previous', 'record', '-', 'holder', 'China', \"'s\", 'Sunway', 'TaihuLight', '.']),\n",
              "       list(['With', 'a', 'peak', 'performance', 'of', '200,000', 'trillion', 'calculations', 'per', 'second', ',', 'it', 'is', 'over', 'twice', 'as', 'fast', 'as', 'Sunway', 'TaihuLight', ',', 'which', 'is', 'capable', 'of', '93,000', 'trillion', 'calculations', 'per', 'second', '.']),\n",
              "       list(['Summit', 'has', '4,608', 'servers', ',', 'which', 'reportedly', 'take', 'up', 'the', 'size', 'of', 'two', 'tennis', 'courts', '.'])],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UObrTgdGKGYa",
        "outputId": "4f105e70-87a6-4e6a-c0bd-acf0fabfc139"
      },
      "source": [
        "words = [word.text for word in text_spacy]\n",
        "np.array(words)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['US', 'unveils', 'world', \"'s\", 'most', 'powerful',\n",
              "       'supercomputer', ',', 'beats', 'China', '.', 'The', 'US', 'has',\n",
              "       'unveiled', 'the', 'world', \"'s\", 'most', 'powerful',\n",
              "       'supercomputer', 'called', \"'\", 'Summit', \"'\", ',', 'beating',\n",
              "       'the', 'previous', 'record', '-', 'holder', 'China', \"'s\",\n",
              "       'Sunway', 'TaihuLight', '.', 'With', 'a', 'peak', 'performance',\n",
              "       'of', '200,000', 'trillion', 'calculations', 'per', 'second', ',',\n",
              "       'it', 'is', 'over', 'twice', 'as', 'fast', 'as', 'Sunway',\n",
              "       'TaihuLight', ',', 'which', 'is', 'capable', 'of', '93,000',\n",
              "       'trillion', 'calculations', 'per', 'second', '.', 'Summit', 'has',\n",
              "       '4,608', 'servers', ',', 'which', 'reportedly', 'take', 'up',\n",
              "       'the', 'size', 'of', 'two', 'tennis', 'courts', '.'], dtype='<U13')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-qnbtAZ0VAE"
      },
      "source": [
        "## Handling Non-Text Characters\n",
        "\n",
        "Natural text consists of various types of characters such as alphabets, numbers, symbols, emoticons, non-printable characters and so on. For most practical use-cases we limit ourselves to alphabets (at most numbers) and ignore other types of characters. \n",
        "\n",
        "In this section, we will focus on identification of non-text characters and how to remove them from our corpus safely.\n",
        "\n",
        "We will focus on handling following types of characters:\n",
        "- Accented Characters\n",
        "- Special Characters\n",
        "- HTML Tags & Noise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzFbsU1e0put"
      },
      "source": [
        "### Accented Characters\n",
        "\n",
        "The most common accents are the acute (é), grave (è), circumflex (â, î or ô), tilde (ñ), umlaut and dieresis (ü or ï – the same symbol is used for two different purposes), and cedilla (ç). Accent marks (also referred to as diacritics or diacriticals) usually appear above a character.\n",
        "\n",
        "These characters are part of extended alphabet in languages such as French, Spanish, etc. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QRMw2Za0VXL"
      },
      "source": [
        "import unicodedata\n",
        "\n",
        "def remove_accented_chars(text):\n",
        "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "    return text"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "guR7PRqX0zEF",
        "outputId": "bc5dc9b3-aa2d-4d13-d947-81d03154232e"
      },
      "source": [
        "s = 'Sómě Áccěntěd těxt'\r\n",
        "s"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Sómě Áccěntěd těxt'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "GipXUT6yLCEy",
        "outputId": "018dfd20-ea8c-4cac-dac5-732f451cb29b"
      },
      "source": [
        "remove_accented_chars(s)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Some Accented text'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3w0miO_i0ewB"
      },
      "source": [
        "### Special Characters\n",
        "\n",
        "Symbols, emoticons and characters such as ``#``, ``@`` etc. are considered special characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rQSOYM40fGG"
      },
      "source": [
        "def remove_special_characters(text, remove_digits=False):\n",
        "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
        "    text = re.sub(pattern, '', text)\n",
        "    return text"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "rRR5f_Vk1OqQ",
        "outputId": "81b275d1-4c16-4df8-fcdb-3fad51172f84"
      },
      "source": [
        "s = \"Well this was fun! See you at 7:30, What do you think!!? #$@@9318@ 🙂🙂🙂\"\r\n",
        "s"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Well this was fun! See you at 7:30, What do you think!!? #$@@9318@ 🙂🙂🙂'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "HL0DRSHYK_Jv",
        "outputId": "5bddaa63-2d3f-44e7-92ce-ff13212414db"
      },
      "source": [
        "remove_special_characters(s, remove_digits=True)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Well this was fun See you at  What do you think  '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "dz-h-z9K1Vk8",
        "outputId": "b6baea20-5a60-41b4-f916-57fbcb61a189"
      },
      "source": [
        "remove_special_characters(s)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Well this was fun See you at 730 What do you think 9318 '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GG1e7Xh1pBJ"
      },
      "source": [
        "### HTML Tags & Noise\n",
        "\n",
        "Many times, NLP datasets are collected as part of web-scraping activities. Web-scraping involves scanning various websites to extract text from them. This process leads to content which is a mix of actual text as well as HTML tags.\n",
        "\n",
        "In this section we will extract HTML version of ** The Bible** book. We will then use ``BeautifulSoup`` to clean out HTML tags to get actual text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcCctZEg1pV2",
        "outputId": "64647e42-f2dd-4a24-f922-ed787b8181f6"
      },
      "source": [
        "data = requests.get(bible_html_url)\n",
        "content = data.text\n",
        "print(content[2745:3948])"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "<p id=\"id00011\" style=\"margin-top: 2em\">*** START OF THE PROJECT GUTENBERG EBOOK, THE BIBLE, KING JAMES, BOOK 1***</p>\r\n",
            "\r\n",
            "<p id=\"id00012\" style=\"margin-top: 4em\">This eBook was produced by David Widger\r\n",
            "with the help of Derek Andrew's text from January 1992\r\n",
            "and the work of Bryan Taylor in November 2002.</p>\r\n",
            "\r\n",
            "<h1 id=\"id00013\" style=\"margin-top: 5em\">Book 01        Genesis</h1>\r\n",
            "\r\n",
            "<p id=\"id00014\">01:001:001 In the beginning God created the heaven and the earth.</p>\r\n",
            "\r\n",
            "<p id=\"id00015\" style=\"margin-left: 0%; margin-right: 0%\">01:001:002 And the earth was without form, and void; and darkness was\r\n",
            "           upon the face of the deep. And the Spirit of God moved upon\r\n",
            "           the face of the waters.</p>\r\n",
            "\r\n",
            "<p id=\"id00016\">01:001:003 And God said, Let there be light: and there was light.</p>\r\n",
            "\r\n",
            "<p id=\"id00017\">01:001:004 And God saw the light, that it was good: and God divided the<br/>\r\n",
            "\r\n",
            "           light from the darkness.<br/>\r\n",
            "</p>\r\n",
            "\r\n",
            "<p id=\"id00018\">01:001:005 And God called the light Day, and the darkness he called<br/>\r\n",
            "\r\n",
            "           Night. And the evening and the morning were the first day.<br/>\r\n",
            "</p>\r\n",
            "\r\n",
            "<p id=\"id00019\">01:001:006 And God said, Let there be a firmament in the m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZ5sHUtvKsZz"
      },
      "source": [
        "def strip_html_tags(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    [s.extract() for s in soup(['iframe', 'script'])]\n",
        "    stripped_text = soup.get_text()\n",
        "    stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
        "    return stripped_text"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJiDezzJKygr",
        "outputId": "d39c3941-3a31-482f-f30f-6bdba1a1a495"
      },
      "source": [
        "clean_content = strip_html_tags(content)\n",
        "print(clean_content[1163:1957])"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*** START OF THE PROJECT GUTENBERG EBOOK, THE BIBLE, KING JAMES, BOOK 1***\n",
            "This eBook was produced by David Widger\n",
            "with the help of Derek Andrew's text from January 1992\n",
            "and the work of Bryan Taylor in November 2002.\n",
            "Book 01        Genesis\n",
            "01:001:001 In the beginning God created the heaven and the earth.\n",
            "01:001:002 And the earth was without form, and void; and darkness was\n",
            "           upon the face of the deep. And the Spirit of God moved upon\n",
            "           the face of the waters.\n",
            "01:001:003 And God said, Let there be light: and there was light.\n",
            "01:001:004 And God saw the light, that it was good: and God divided the\n",
            "           light from the darkness.\n",
            "01:001:005 And God called the light Day, and the darkness he called\n",
            "           Night. And the evening and the morning were the first day.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LxLCnCFxlpf"
      },
      "source": [
        "\n",
        "That seemed to have worked like a charm!\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEegW2cg0yiZ"
      },
      "source": [
        "## Text Correction\n",
        "\n",
        "In this section, we will prepare utilities to fix different issues with textual data.\n",
        "\n",
        "- Fix Repeat Characters\n",
        "- Spell Correction\n",
        "- Expand Contractions\n",
        "- Stemming\n",
        "- Lemmatization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPQwNp-g1Lc_"
      },
      "source": [
        "### Repeat Characters\n",
        "\n",
        "Repeating characters are a common mistake in textual data. We will look at different techniques to handle the same."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxfNK0MN01fY"
      },
      "source": [
        "old_word = 'finalllyyy'"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmztnK0lyeD0"
      },
      "source": [
        "#### Base Version: Regex pattern to search repeating characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOmMTqfyLck5"
      },
      "source": [
        "repeat_pattern = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
        "match_substitution = r'\\1\\2\\3'"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4ewE1QnLen8",
        "outputId": "eca664bf-8cfd-484c-d3a1-3b49275886c8"
      },
      "source": [
        "step = 1\n",
        "\n",
        "while True:\n",
        "    # remove one repeated character\n",
        "    new_word = repeat_pattern.sub(match_substitution,\n",
        "                                  old_word)\n",
        "    if new_word != old_word:\n",
        "         print('Step: {} Word: {}'.format(step, new_word))\n",
        "         step += 1 # update step\n",
        "         # update old word to last substituted state\n",
        "         old_word = new_word  \n",
        "         continue\n",
        "    else:\n",
        "         print(\"Final word:\", new_word)\n",
        "         break"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step: 1 Word: finalllyy\n",
            "Step: 2 Word: finallly\n",
            "Step: 3 Word: finally\n",
            "Step: 4 Word: finaly\n",
            "Final word: finaly\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3PpUcaDyn07"
      },
      "source": [
        "#### Enhanced Version: Use Wordnet to check if new word is a dictionary word to stop repeated text removal at the right point."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fRn85TwLnJO"
      },
      "source": [
        "from nltk.corpus import wordnet"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akayz381Li0u"
      },
      "source": [
        "old_word = 'finalllyyy'"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwUGRFTmLis0"
      },
      "source": [
        "repeat_pattern = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
        "match_substitution = r'\\1\\2\\3'"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_UGpr5RLpqa",
        "outputId": "191abd70-b280-4b13-f8eb-be38b996184d"
      },
      "source": [
        "step = 1\n",
        " \n",
        "while True:\n",
        "    # check for semantically correct word\n",
        "    if wordnet.synsets(old_word):\n",
        "        print(\"Final correct word:\", old_word)\n",
        "        break\n",
        "    # remove one repeated character\n",
        "    new_word = repeat_pattern.sub(match_substitution,\n",
        "                                  old_word)\n",
        "    if new_word != old_word:\n",
        "        print('Step: {} Word: {}'.format(step, new_word))\n",
        "        step += 1 # update step\n",
        "        # update old word to last substituted state\n",
        "        old_word = new_word  \n",
        "        continue\n",
        "    else:\n",
        "        print(\"Final word:\", new_word)\n",
        "        break"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step: 1 Word: finalllyy\n",
            "Step: 2 Word: finallly\n",
            "Step: 3 Word: finally\n",
            "Final correct word: finally\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CphRh71vy5kG"
      },
      "source": [
        "#### Optimized Version: Use regex and wordnet to perform cleanup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOwMJpdILuAt"
      },
      "source": [
        "def remove_repeated_characters(tokens):\n",
        "    repeat_pattern = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
        "    match_substitution = r'\\1\\2\\3'\n",
        "    def replace(old_word):\n",
        "        if wordnet.synsets(old_word):\n",
        "            return old_word\n",
        "        new_word = repeat_pattern.sub(match_substitution, old_word)\n",
        "        return replace(new_word) if new_word != old_word else new_word\n",
        "            \n",
        "    correct_tokens = [replace(word) for word in tokens]\n",
        "    return correct_tokens"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "YZQOU8QkLt9u",
        "outputId": "126fe37d-e9d0-48a9-9315-ba044da72423"
      },
      "source": [
        "sample_sentence = 'This tutorial is realllllyyy amaaazingggg'\n",
        "correct_tokens = remove_repeated_characters(nltk.word_tokenize(sample_sentence))\n",
        "' '.join(correct_tokens)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'This tutorial is really amazing'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cixy53Id14u5"
      },
      "source": [
        "_Note:_ This may fail for some edge cases based on order of word removals, spell check is an option after this to get the correct word."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CKJyc6S1Oib"
      },
      "source": [
        "### Spell Correction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oxb-SZwtz_SK"
      },
      "source": [
        "#### Base Version:\n",
        "1. Prepare a list of words along with their frequency (you can use any corpora for this having correct spelled out words, more the better. The corpora we use here is the same also referenced in _Chapter 3 of Text Analytics with Python 2nd ed._)\n",
        "2. Prepare utility functions to calculate edit-distance of 0, 1 and 2 based on possible replacements of individual characters in a given word\n",
        "3. Search for replacements in the word-frequency list for possible corrections"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPs7TWN-3YTq"
      },
      "source": [
        "spell_corpus_resp = requests.get('https://github.com/dipanjanS/text-analytics-with-python/blob/master/New-Second-Edition/Ch03%20-%20Processing%20and%20Understanding%20Text/big.txt?raw=true')\r\n",
        "spell_corpus = spell_corpus_resp.text"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "VkMMR4Vt3gm8",
        "outputId": "c47e6524-6d36-4d9b-c32d-5b047e024d8f"
      },
      "source": [
        "spell_corpus[:100]"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The Project Gutenberg EBook of The Adventures of Sherlock Holmes\\nby Sir Arthur Conan Doyle\\n(#15 in o'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "db3sREWc1O1_"
      },
      "source": [
        "import collections"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2NTeMstL5wu"
      },
      "source": [
        "def tokens(text): \n",
        "    \"\"\"\n",
        "    Get all words from the corpus\n",
        "    \"\"\"\n",
        "    return re.findall('[a-z]+', text.lower()) "
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYvESWG4L5tN",
        "outputId": "f97b54fe-5ba0-4e37-de31-4e22f8b3363c"
      },
      "source": [
        "WORDS = tokens(spell_corpus)\n",
        "WORD_COUNTS = collections.Counter(WORDS)\n",
        "# top 10 words in corpus\n",
        "WORD_COUNTS.most_common(10)"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('the', 80030),\n",
              " ('of', 40025),\n",
              " ('and', 38313),\n",
              " ('to', 28766),\n",
              " ('in', 22050),\n",
              " ('a', 21155),\n",
              " ('that', 12512),\n",
              " ('he', 12401),\n",
              " ('was', 11410),\n",
              " ('it', 10681)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYRGBJdIMCQ_"
      },
      "source": [
        " def edits0(word): \n",
        "    \"\"\"\n",
        "    Return all strings that are zero edits away \n",
        "    from the input word (i.e., the word itself).\n",
        "    \"\"\"\n",
        "    return {word}\n",
        "\n",
        "\n",
        "\n",
        "def edits1(word):\n",
        "    \"\"\"\n",
        "    Return all strings that are one edit away \n",
        "    from the input word.\n",
        "    \"\"\"\n",
        "    alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    def splits(word):\n",
        "        \"\"\"\n",
        "        Return a list of all possible (first, rest) pairs \n",
        "        that the input word is made of.\n",
        "        \"\"\"\n",
        "        return [(word[:i], word[i:]) \n",
        "                for i in range(len(word)+1)]\n",
        "                \n",
        "    pairs      = splits(word)\n",
        "    deletes    = [a+b[1:]           for (a, b) in pairs if b]\n",
        "    transposes = [a+b[1]+b[0]+b[2:] for (a, b) in pairs if len(b) > 1]\n",
        "    replaces   = [a+c+b[1:]         for (a, b) in pairs for c in alphabet if b]\n",
        "    inserts    = [a+c+b             for (a, b) in pairs for c in alphabet]\n",
        "    return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "\n",
        "def edits2(word):\n",
        "    \"\"\"Return all strings that are two edits away \n",
        "    from the input word.\n",
        "    \"\"\"\n",
        "    return {e2 for e1 in edits1(word) for e2 in edits1(e1)}"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6Av8TztMCIO"
      },
      "source": [
        "def known(words):\n",
        "    \"\"\"\n",
        "    Return the subset of words that are actually \n",
        "    in our WORD_COUNTS dictionary.\n",
        "    \"\"\"\n",
        "    return {w for w in words if w in WORD_COUNTS}"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKDh1aBtMLLe",
        "outputId": "ff09b613-94ae-496c-ef0f-57b6b285035b"
      },
      "source": [
        "# input word\n",
        "In [409]: word = 'fianlly'\n",
        "\n",
        "# zero edit distance from input word\n",
        "edits0(word)"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'fianlly'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VdymaShTMOsV",
        "outputId": "8b7a1b1c-f2b1-403c-cfb2-433dfcc8f8cd"
      },
      "source": [
        "# returns null set since it is not a valid word\n",
        "known(edits0(word))"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "set()"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpmycTdVMTkn",
        "outputId": "e7c66bbd-4e33-4c2a-c7e5-000a07e071e3"
      },
      "source": [
        "# one edit distance from input word\n",
        "list(edits1(word))[:10]"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['fianlmy',\n",
              " 'fiqnlly',\n",
              " 'fiaxlly',\n",
              " 'sianlly',\n",
              " 'ofianlly',\n",
              " 'fianldly',\n",
              " 'yianlly',\n",
              " 'fdanlly',\n",
              " 'fianlldy',\n",
              " 'fbianlly']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZ37lYobMTeh",
        "outputId": "b2fc28c7-ef5a-420b-c2be-b81d36c60ef6"
      },
      "source": [
        "# get correct words from above set\n",
        "known(edits1(word))"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'finally'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VyXQ3vTTMgj8",
        "outputId": "ebb53011-b0c7-48e2-ad04-382a7c709d0e"
      },
      "source": [
        "# two edit distances from input word\n",
        "list(edits2(word))[:10]"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['fianljlm',\n",
              " 'fianlxgy',\n",
              " 'fwiranlly',\n",
              " 'tianllk',\n",
              " 'sfcianlly',\n",
              " 'fhanllty',\n",
              " 'fiaubly',\n",
              " 'fialnllyp',\n",
              " 'piadnlly',\n",
              " 'fiancwy']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjIsBMyRMghN",
        "outputId": "93d464ed-4f2b-4805-ab90-792847d68af1"
      },
      "source": [
        "# get correct words from above set\n",
        "known(edits2(word))"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'faintly', 'finally', 'finely', 'frankly'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xiA5eeloMgew",
        "outputId": "93cc5715-dd68-4007-e736-02dba6e78697"
      },
      "source": [
        "candidates = (known(edits0(word)) or \n",
        "              known(edits1(word)) or \n",
        "              known(edits2(word)) or \n",
        "              [word])\n",
        "candidates"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'finally'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKJDuz69MgcC"
      },
      "source": [
        "def correct(word):\n",
        "    \"\"\"\n",
        "    Get the best correct spelling for the input word\n",
        "    \"\"\"\n",
        "    # Priority is for edit distance 0, then 1, then 2\n",
        "    # else defaults to the input word itself.\n",
        "    candidates = (known(edits0(word)) or \n",
        "                  known(edits1(word)) or \n",
        "                  known(edits2(word)) or \n",
        "                  [word])\n",
        "    return max(candidates, key=WORD_COUNTS.get)"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "C1rqrfV3Mvxc",
        "outputId": "e39f9c81-f83a-4a55-8a23-821eb1be5f90"
      },
      "source": [
        "correct('fianlly')"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'finally'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Br_bsrNYz6mU"
      },
      "source": [
        "#### Improved Version:\n",
        "Use ``textblob`` to find correct word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "d_wBcnGtMvr8",
        "outputId": "5454878f-87ee-4af3-9395-6f1a535d6d68"
      },
      "source": [
        "from textblob import Word\n",
        "\n",
        "w = Word('fianlly')\n",
        "w.correct()"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'finally'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBvPgy75PFBM",
        "outputId": "d8f77372-93ab-49ff-acd0-bcc34df63fe6"
      },
      "source": [
        "w.spellcheck()"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('finally', 1.0)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fSb0COHPJN2",
        "outputId": "660afaea-c7bd-494d-9aff-1f4a7113e0f5"
      },
      "source": [
        "w = Word('flaot')\n",
        "w.spellcheck()"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('flat', 0.85), ('float', 0.15)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e_cotz81RKq"
      },
      "source": [
        "## Stemming\n",
        "\n",
        "In linguistic morphology and information retrieval, stemming is the process of reducing inflected words to their word stem, base or root form—generally a written word form"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwjTfalN0U8t"
      },
      "source": [
        "#### Porter Stemmer\n",
        "\n",
        "The Porter stemming algorithm (or 'Porter stemmer') is a process for removing the commoner morphological and inflexional endings from words in English."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSphmzxS1Slq"
      },
      "source": [
        "from nltk.stem import PorterStemmer"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znVyq3LLPQh_",
        "outputId": "1000da31-c32f-4098-e835-d35b2800b4e6"
      },
      "source": [
        "ps = PorterStemmer()\n",
        "\n",
        "ps.stem('jumping'), ps.stem('jumps'), ps.stem('jumped')"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('jump', 'jump', 'jump')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "PcKf-pNkQGKy",
        "outputId": "449be1b3-3a61-4fb4-b802-6f4c1853bb5f"
      },
      "source": [
        "ps.stem('lying')"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'lie'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Wn-IMSgCQF5E",
        "outputId": "5811f5b4-34d5-459d-8ad2-ce89fe5a7ddb"
      },
      "source": [
        "ps.stem('strange')"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'strang'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIgS4XM00nak"
      },
      "source": [
        "#### Lancaster Stemmer\n",
        "The Lancaster stemmers are more aggressive and dynamic compared to the other two stemmers. The stemmer is really faster, but the algorithm is really confusing when dealing with small words. But they are not as efficient as Snowball Stemmers. The Lancaster stemmers save the rules externally and basically uses an iterative algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nc0Mas_kQObP",
        "outputId": "5caa06ec-3af9-4449-e7ed-665f4d7be1d0"
      },
      "source": [
        "from nltk.stem import LancasterStemmer\n",
        "ls = LancasterStemmer()\n",
        "\n",
        "ls.stem('jumping'), ls.stem('jumps'), ls.stem('jumped')"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('jump', 'jump', 'jump')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "IyYoJWvuQPYO",
        "outputId": "bef7629b-b6bb-4fa1-d2c4-e70dc8e08214"
      },
      "source": [
        "ls.stem('lying')"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'lying'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-_t_nRSoQPO1",
        "outputId": "21e4fff6-817f-42ac-f282-8bedb4fc8ece"
      },
      "source": [
        "ls.stem('strange')"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'strange'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPjegsFA093F"
      },
      "source": [
        "#### Regex based stemmer\n",
        "\n",
        "Regular Expression based Stemmer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAfXiue5QWmL",
        "outputId": "0d0bb601-a8f6-429a-86de-d0f513eafbd1"
      },
      "source": [
        "from nltk.stem import RegexpStemmer\n",
        "rs = RegexpStemmer('ing$|s$|ed$', min=4)\n",
        "rs.stem('jumping'), rs.stem('jumps'), rs.stem('jumped')"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('jump', 'jump', 'jump')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "vYz7yNdwQWjA",
        "outputId": "f1ab8904-80ee-4eaf-d160-77eb133f64e2"
      },
      "source": [
        "rs.stem('lying')"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'ly'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trPCF2J61Cj5"
      },
      "source": [
        "#### Snowball Stemmer\n",
        "\n",
        "Snowball is a small string processing language for creating stemming algorithms for use in Information Retrieval, plus a collection of stemming algorithms implemented using it.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZzVzt8LHQWd4",
        "outputId": "e24c4d59-9ed8-4554-e19a-83c1bfbbfcb8"
      },
      "source": [
        "# Snowball Stemmer\n",
        "from nltk.stem import SnowballStemmer\n",
        "ss = SnowballStemmer(\"german\")\n",
        "print('Supported Languages:', SnowballStemmer.languages)"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Supported Languages: ('arabic', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'porter', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "DQsoQ58_QWKI",
        "outputId": "f7e2127a-6341-45b4-d222-6abd4c671cb3"
      },
      "source": [
        "# stemming on German words\n",
        "# autobahnen -> cars\n",
        "# autobahn -> car\n",
        "ss.stem('autobahnen')"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'autobahn'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zt9y8f0ZQWAb"
      },
      "source": [
        "ps = nltk.porter.PorterStemmer()\n",
        "ls = nltk.stem.LancasterStemmer()\n",
        "\n",
        "def simple_stemmer(text, stemmer=ps):\n",
        "    text = ' '.join([stemmer.stem(word) for word in text.split()])\n",
        "    return text"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQBpxXCb4eGh"
      },
      "source": [
        "#### Try calling the above defined function for both Lancaster and Porter stemmer separately"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-RK-0_Pu4rmm",
        "outputId": "c2bc918f-6232-4675-a20b-5505a5e4417e"
      },
      "source": [
        "s = \"My system keeps crashing his crashed yesterday ours crashes daily and presumably we are not lying\"\r\n",
        "s"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'My system keeps crashing his crashed yesterday ours crashes daily and presumably we are not lying'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "A-gWD0o5QVnp",
        "outputId": "81513bc1-1ab6-404a-9bc2-20d2c3eb87bf"
      },
      "source": [
        "simple_stemmer(s, stemmer=ps)"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'My system keep crash hi crash yesterday our crash daili and presum we are not lie'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "mHM0-NyF4xgi",
        "outputId": "74dc2c46-b62a-4be5-dfb4-ea6ff741e390"
      },
      "source": [
        "simple_stemmer(s, stemmer=ls)"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'my system keep crash his crash yesterday our crash dai and presum we ar not lying'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gh2VFpaT1TKx"
      },
      "source": [
        "## Lemmatization\n",
        "\n",
        "Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5J2DLu41Ues"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "wnl = WordNetLemmatizer()"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_y8ZYpk1VPe",
        "outputId": "b5ede5c9-6fb4-4e97-fa15-62d8002ac5bd"
      },
      "source": [
        "# lemmatize nouns\n",
        "print(wnl.lemmatize('cars', 'n'))\n",
        "print(wnl.lemmatize('men', 'n'))"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "car\n",
            "men\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "796cg1bWQtII",
        "outputId": "6c4adaa3-6875-4bd0-a73d-21d8de1b4ef1"
      },
      "source": [
        "# lemmatize verbs\n",
        "print(wnl.lemmatize('running', 'v'))\n",
        "print(wnl.lemmatize('ate', 'v'))"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "run\n",
            "eat\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0-Pt7hL455X",
        "outputId": "cead7de7-3105-406f-94a5-d8cc5032585b"
      },
      "source": [
        "# lemmatize adjectives\r\n",
        "print(wnl.lemmatize('saddest', 'a'))\r\n",
        "print(wnl.lemmatize('fancier', 'a'))"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sad\n",
            "fancy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTgZtWht46AT",
        "outputId": "04388f25-7f23-4d95-a6fb-5679eec8d108"
      },
      "source": [
        "# ineffective lemmatization\r\n",
        "print(wnl.lemmatize('ate', 'n'))\r\n",
        "print(wnl.lemmatize('fancier', 'v'))\r\n",
        "print(wnl.lemmatize('fancier'))"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ate\n",
            "fancier\n",
            "fancier\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4ifadT65PZU"
      },
      "source": [
        "#### Building your own lemmatizer using nltk\r\n",
        "\r\n",
        "Define a function such that you put all the above steps together so that it does the following\r\n",
        "- Function name is `wordnet_lemmatize_text(...)`\r\n",
        "- Input is a variable text which should take in a document (bunch of words)\r\n",
        "- Need to tokenize the text\r\n",
        "- Get POS tags of tokenized text\r\n",
        "- Convert POS tags into wordnet (single letter) POS tags\r\n",
        "- use nltk's wordnet lemmatizer\r\n",
        "- Return lemmatized text as the output (as a string)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G20-siNJ5nFt"
      },
      "source": [
        "from nltk.corpus import wordnet\r\n",
        "wnl = WordNetLemmatizer()\r\n",
        "\r\n",
        "def wordnet_lemmatize_text(text):\r\n",
        "  # tokenize text\r\n",
        "  tokens = nltk.word_tokenize(text)\r\n",
        "  # pos tag tokenized text\r\n",
        "  tagged_tokens = nltk.pos_tag(tokens)\r\n",
        "  # convert raw POS tags into wordnet tags\r\n",
        "  tag_map = {'j': wordnet.ADJ, 'v': wordnet.VERB, 'n': wordnet.NOUN, 'r': wordnet.ADV}\r\n",
        "  # treat unknown tags as nouns by default\r\n",
        "  new_tagged_tokens = [(word, tag_map.get(tag[0].lower(), \r\n",
        "                                          wordnet.NOUN))\r\n",
        "                            for word, tag in tagged_tokens]\r\n",
        "\r\n",
        "  lemmatized_text = ' '.join(wnl.lemmatize(word, tag) for word, tag in new_tagged_tokens)\r\n",
        "  return lemmatized_text"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "jAS_a-cu6Obm",
        "outputId": "b8adc41f-4b01-4b92-8501-3af93ab649eb"
      },
      "source": [
        "s = 'The brown foxes are quick and they are jumping over the sleeping lazy dogs!'\r\n",
        "s"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The brown foxes are quick and they are jumping over the sleeping lazy dogs!'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "pGL2hRDR6TFy",
        "outputId": "56a8ac7c-be3c-4bfa-fcc1-a2a0f39fa902"
      },
      "source": [
        "wordnet_lemmatize_text(s)"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The brown fox be quick and they be jump over the sleep lazy dog !'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LaWjzXYQzFM"
      },
      "source": [
        "#### Spacy Lemmatization\r\n",
        "\r\n",
        "Out of the box implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8KMWjKtQwEd"
      },
      "source": [
        "import spacy\n",
        "# use spacy.load('en') if you have downloaded the language model en directly after install spacy\n",
        "nlp = spacy.load('en')\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    text = nlp(text)\n",
        "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
        "    return text"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "qGhiUFpFQ4B_",
        "outputId": "36f27dd5-d21c-4e2f-ccde-9af41ad573b6"
      },
      "source": [
        "lemmatize_text(s)"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'the brown fox be quick and they be jump over the sleep lazy dog !'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63RQADG9RCDT"
      },
      "source": [
        "## Stopword Removal\n",
        "\n",
        "In computing, stop words are words which are filtered out before or after processing of natural language data. A stop word is a commonly used word (such as “the”, “a”, “an”,etc.) which does not convey a lot of useful information\n",
        "\n",
        "We typically remove stopwords before using text for most NLP tasks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZCYbpMrQ9GR"
      },
      "source": [
        "def remove_stopwords(text, is_lower_case=False, stopwords=None):\n",
        "    if not stopwords:\n",
        "        stopwords = nltk.corpus.stopwords.words('english')\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tokens = [token.strip() for token in tokens]\n",
        "    \n",
        "    if is_lower_case:\n",
        "        filtered_tokens = [token for token in tokens if token not in stopwords]\n",
        "    else:\n",
        "        filtered_tokens = [token for token in tokens if token.lower() not in stopwords]\n",
        "    \n",
        "    filtered_text = ' '.join(filtered_tokens)    \n",
        "    return filtered_text"
      ],
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVfF6BUY7AgI",
        "outputId": "189592ab-fbfe-4b50-eb5b-b147b8652417"
      },
      "source": [
        "stop_words = nltk.corpus.stopwords.words('english')\r\n",
        "print(stop_words[:10])"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "bo2AVwoARIm3",
        "outputId": "eb12f16a-5547-4cd2-fa4c-20f1f58f7064"
      },
      "source": [
        "s"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The brown foxes are quick and they are jumping over the sleeping lazy dogs!'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "BrZP5DCp7D1c",
        "outputId": "8b92867a-5096-4391-a6b6-46d00ab7364f"
      },
      "source": [
        "remove_stopwords(s, is_lower_case=False)"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'brown foxes quick jumping sleeping lazy dogs !'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3-IWN2s7I2o"
      },
      "source": [
        "Remove the word 'the' and add the word 'brown' from the stop_words list and call the function with this new list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiLGzqCG7Jt2"
      },
      "source": [
        "stop_words.remove('the')\r\n",
        "stop_words.append('brown')"
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "DbiyGZEm7LRz",
        "outputId": "095d24f9-eec1-4fca-bbb4-980f6d0baf16"
      },
      "source": [
        "remove_stopwords(s, is_lower_case=False, stopwords=stop_words)"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The foxes quick jumping the sleeping lazy dogs !'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9NJ07FQRXWO"
      },
      "source": [
        "## Expand Contractions\n",
        "\n",
        "Contractions are words or combinations of words that are shortened by dropping letters and replacing them by an apostrophe.\n",
        "\n",
        "In order to capture context better, we standardize text by expanding such contractions. ``contractions`` and ``textsearch`` enable us to do so in just a few lines of code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWUKQuIhROAt",
        "outputId": "26bf6c4a-b672-4730-fff8-079bc1cbb0f7"
      },
      "source": [
        "!pip install contractions\n",
        "!pip install textsearch"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting contractions\n",
            "  Downloading https://files.pythonhosted.org/packages/4a/5f/91102df95715fdda07f56a7eba2baae983e2ae16a080eb52d79e08ec6259/contractions-0.0.45-py2.py3-none-any.whl\n",
            "Collecting textsearch\n",
            "  Downloading https://files.pythonhosted.org/packages/42/a8/03407021f9555043de5492a2bd7a35c56cc03c2510092b5ec018cae1bbf1/textsearch-0.0.17-py2.py3-none-any.whl\n",
            "Collecting Unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/65/91eab655041e9e92f948cb7302e54962035762ce7b518272ed9d6b269e93/Unidecode-1.1.2-py2.py3-none-any.whl (239kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 8.5MB/s \n",
            "\u001b[?25hCollecting pyahocorasick\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/9f/f0d8e8850e12829eea2e778f1c90e3c53a9a799b7f412082a5d21cd19ae1/pyahocorasick-1.4.0.tar.gz (312kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 8.3MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
            "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.0-cp36-cp36m-linux_x86_64.whl size=81699 sha256=37d2f38e31c8e3dc63cf032e427c5f2c4a1e103cf6d82f6c3b2b88dbab4a6b89\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/90/61/87a55f5b459792fbb2b7ba6b31721b06ff5cf6bde541b40994\n",
            "Successfully built pyahocorasick\n",
            "Installing collected packages: Unidecode, pyahocorasick, textsearch, contractions\n",
            "Successfully installed Unidecode-1.1.2 contractions-0.0.45 pyahocorasick-1.4.0 textsearch-0.0.17\n",
            "Requirement already satisfied: textsearch in /usr/local/lib/python3.6/dist-packages (0.0.17)\n",
            "Requirement already satisfied: Unidecode in /usr/local/lib/python3.6/dist-packages (from textsearch) (1.1.2)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.6/dist-packages (from textsearch) (1.4.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qyYeGwhRlPL"
      },
      "source": [
        "import contractions"
      ],
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bc41Ls6KRglA",
        "outputId": "02aae52d-9bfe-4cba-95fc-3919967f915e"
      },
      "source": [
        "list(contractions.contractions_dict.items())[:10]"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(\"I'm\", 'I am'),\n",
              " (\"I'm'a\", 'I am about to'),\n",
              " (\"I'm'o\", 'I am going to'),\n",
              " (\"I've\", 'I have'),\n",
              " (\"I'll\", 'I will'),\n",
              " (\"I'll've\", 'I will have'),\n",
              " (\"I'd\", 'I would'),\n",
              " (\"I'd've\", 'I would have'),\n",
              " ('Whatcha', 'What are you'),\n",
              " (\"amn't\", 'am not')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0V-aX01iRcmW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8ef13311-888b-43a6-8e76-16c62fde2bc0"
      },
      "source": [
        "sample_string = \"Y'all can't expand contractions I'd think! You wouldn't be able to. How'd you do it?\"\r\n",
        "sample_string"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Y'all can't expand contractions I'd think! You wouldn't be able to. How'd you do it?\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "hNR1iYWlRoVc",
        "outputId": "1280b143-9593-4f2c-c988-9cb2661d2203"
      },
      "source": [
        "contractions.fix(sample_string)"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'you all can not expand contractions I would think! You would not be able to. how did you do it?'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 142
        }
      ]
    }
  ]
}