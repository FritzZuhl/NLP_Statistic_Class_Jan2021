{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "Tutorial 02 - Build your own Text Pre-processor.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsQzYZL28h8i"
      },
      "source": [
        "# Tutorial 2: Build your own text pre-processor\r\n",
        "\r\n",
        "This shows a sequence of common pre-processing functions you can use to build your own text pre-processor.\r\n",
        "\r\n",
        "We also show you how to build a fast text pre-processor which can use multi-threading if your CPU is fast enough.\r\n",
        "\r\n",
        "__Note:__ There is no perfect set of pre-processing steps and it will depend based on the problem at hand and also by trying and viewing the results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAayJMwxMk3k"
      },
      "source": [
        "# Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWXZ53D5MnRR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d7050ef-0bca-49df-ebf0-eded642128d4"
      },
      "source": [
        "!pip install textsearch\n",
        "!pip install contractions\n",
        "!pip install tqdm\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting textsearch\n",
            "  Downloading https://files.pythonhosted.org/packages/42/a8/03407021f9555043de5492a2bd7a35c56cc03c2510092b5ec018cae1bbf1/textsearch-0.0.17-py2.py3-none-any.whl\n",
            "Collecting pyahocorasick\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/9f/f0d8e8850e12829eea2e778f1c90e3c53a9a799b7f412082a5d21cd19ae1/pyahocorasick-1.4.0.tar.gz (312kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 8.0MB/s \n",
            "\u001b[?25hCollecting Unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/65/91eab655041e9e92f948cb7302e54962035762ce7b518272ed9d6b269e93/Unidecode-1.1.2-py2.py3-none-any.whl (239kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 12.0MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
            "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.0-cp36-cp36m-linux_x86_64.whl size=81697 sha256=5872ba40dd08acb8c2a58c6442c9d4ee39aecc2b893ade734032031f0a1b9070\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/90/61/87a55f5b459792fbb2b7ba6b31721b06ff5cf6bde541b40994\n",
            "Successfully built pyahocorasick\n",
            "Installing collected packages: pyahocorasick, Unidecode, textsearch\n",
            "Successfully installed Unidecode-1.1.2 pyahocorasick-1.4.0 textsearch-0.0.17\n",
            "Collecting contractions\n",
            "  Downloading https://files.pythonhosted.org/packages/4a/5f/91102df95715fdda07f56a7eba2baae983e2ae16a080eb52d79e08ec6259/contractions-0.0.45-py2.py3-none-any.whl\n",
            "Requirement already satisfied: textsearch in /usr/local/lib/python3.6/dist-packages (from contractions) (0.0.17)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.6/dist-packages (from textsearch->contractions) (1.4.0)\n",
            "Requirement already satisfied: Unidecode in /usr/local/lib/python3.6/dist-packages (from textsearch->contractions) (1.1.2)\n",
            "Installing collected packages: contractions\n",
            "Successfully installed contractions-0.0.45\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.41.1)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0G_9oc6ICdU"
      },
      "source": [
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import unicodedata\n",
        "import contractions\n",
        "import spacy\n",
        "import nltk\n",
        "\n",
        "nlp = spacy.load('en')\n",
        "ps = nltk.porter.PorterStemmer()\n",
        "\n",
        "# HTML removal\n",
        "def strip_html_tags(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    [s.extract() for s in soup(['iframe', 'script'])]\n",
        "    stripped_text = soup.get_text()\n",
        "    stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
        "    return stripped_text\n",
        "\n",
        "# accent removal\n",
        "def remove_accented_chars(text):\n",
        "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "    return text\n",
        "\n",
        "# contraction expansion\n",
        "def expand_contractions(text):\n",
        "    return contractions.fix(text)\n",
        "\n",
        "# lemamtization\n",
        "def spacy_lemmatize_text(text):\n",
        "    text = nlp(text)\n",
        "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
        "    return text\n",
        "\n",
        "# stemming\n",
        "def simple_stemming(text, stemmer=ps):\n",
        "    text = ' '.join([stemmer.stem(word) for word in text.split()])\n",
        "    return text\n",
        "\n",
        "# special character removal\n",
        "def remove_special_characters(text, remove_digits=False):\n",
        "    pattern = r'[^a-zA-Z0-9\\s]' if not remove_digits else r'[^a-zA-Z\\s]'\n",
        "    text = re.sub(pattern, '', text)\n",
        "    return text\n",
        "\n",
        "# stopword removal\n",
        "def remove_stopwords(text, is_lower_case=False, stopwords=None):\n",
        "    if not stopwords:\n",
        "        stopwords = nltk.corpus.stopwords.words('english')\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tokens = [token.strip() for token in tokens]\n",
        "    \n",
        "    if is_lower_case:\n",
        "        filtered_tokens = [token for token in tokens if token not in stopwords]\n",
        "    else:\n",
        "        filtered_tokens = [token for token in tokens if token.lower() not in stopwords]\n",
        "    \n",
        "    filtered_text = ' '.join(filtered_tokens)    \n",
        "    return filtered_text"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwA-saFr92RT"
      },
      "source": [
        "Handling extra newlines and carriage returns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aloz8FEcICdY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80c8bbe9-8aa3-4ab4-a16e-a1f130d5fd12"
      },
      "source": [
        "s = 'hello\\r\\nhow are you doing\\r\\nI\\tam\\tdoing\\tgreat\\r\\n:)'\n",
        "print(s)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hello\r\n",
            "how are you doing\r\n",
            "I\tam\tdoing\tgreat\r\n",
            ":)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lvBAtV9ICdb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d85f41c4-a72e-46db-c8eb-e3965f671aef"
      },
      "source": [
        "s.translate(s.maketrans(\"\\n\\t\\r\", \"   \"))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'hello  how are you doing  I am doing great  :)'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34cnm0OkICdd"
      },
      "source": [
        "## Your Turn: Add in all the necessary functions and build your pre-processor!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_N5OiA07ICdd"
      },
      "source": [
        "import tqdm # for nice progressbar\n",
        "\n",
        "def text_pre_processor(text, html_strip=True, accented_char_removal=True, contraction_expansion=True,\n",
        "                       text_lower_case=True, text_stemming=False, text_lemmatization=True, \n",
        "                       special_char_removal=True, remove_digits=True, stopword_removal=True, \n",
        "                       stopword_list=None):\n",
        "    \n",
        "    # strip HTML\n",
        "    if html_strip:\n",
        "        text = strip_html_tags(text)\n",
        "    \n",
        "    # remove extra newlines (often might be present in really noisy text)\n",
        "    text = text.translate(text.maketrans(\"\\n\\t\\r\", \"   \"))\n",
        "    \n",
        "    # remove accented characters\n",
        "    if accented_char_removal:\n",
        "        text = remove_accented_chars(text)\n",
        "    \n",
        "    # expand contractions    \n",
        "    if contraction_expansion:\n",
        "        text = expand_contractions(text)\n",
        "        \n",
        "    \n",
        "    # lemmatize text\n",
        "    if text_lemmatization:\n",
        "        text = spacy_lemmatize_text(text) \n",
        "        \n",
        "    # remove special characters and\\or digits    \n",
        "    if special_char_removal:\n",
        "        # insert spaces between special characters to isolate them    \n",
        "        special_char_pattern = re.compile(r'([{.(-)!}])')\n",
        "        text = special_char_pattern.sub(\" \\\\1 \", text)\n",
        "        text = remove_special_characters(text, remove_digits=remove_digits)  \n",
        "        \n",
        "    # stem text\n",
        "    if text_stemming and not text_lemmatization:\n",
        "        text = simple_stemming(text)\n",
        "        \n",
        "    # lowercase the text    \n",
        "    if text_lower_case:\n",
        "        text = text.lower()\n",
        "        \n",
        "        \n",
        "    # remove stopwords\n",
        "    if stopword_removal:\n",
        "        text = remove_stopwords(text, is_lower_case=text_lower_case, \n",
        "                                stopwords=stopword_list)\n",
        "        \n",
        "    # remove extra whitespace\n",
        "    text = re.sub(' +', ' ', text)\n",
        "    text = text.strip()\n",
        "    \n",
        "    return text\n",
        "\n",
        "  \n",
        "def corpus_pre_processor(corpus):\n",
        "  norm_corpus = []\n",
        "  for doc in tqdm.tqdm(corpus):\n",
        "    norm_corpus.append(text_pre_processor(doc))\n",
        "  return norm_corpus"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBE3P53iICdg"
      },
      "source": [
        "# Test on a single document"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjL4Mx9aICdg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ec4774e-38c2-4b45-bc9f-489cf0944312"
      },
      "source": [
        "document = \"\"\"<p>Héllo! Héllo! can you hear me! I just heard about <b>Python</b>!<br/>\\r\\n \n",
        "              It's an amazing language which can be used for [Scripting\\tWeb development\\tBackend development],\\r\\n\\r\\n\n",
        "              Information Retrieval, Natural Language Processing, Machine Learning & Artificial Intelligence!\\n\n",
        "              What are you waiting for? Go and get started.<br/> He's learning, she's learning, they've already\\n\\n\n",
        "              got a headstart! GET PYTHON 3.6 NOW!</p>\n",
        "           \"\"\"\n",
        "print(document)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<p>Héllo! Héllo! can you hear me! I just heard about <b>Python</b>!<br/>\r\n",
            " \n",
            "              It's an amazing language which can be used for [Scripting\tWeb development\tBackend development],\r\n",
            "\r\n",
            "\n",
            "              Information Retrieval, Natural Language Processing, Machine Learning & Artificial Intelligence!\n",
            "\n",
            "              What are you waiting for? Go and get started.<br/> He's learning, she's learning, they've already\n",
            "\n",
            "\n",
            "              got a headstart! GET PYTHON 3.6 NOW!</p>\n",
            "           \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q67MCFxLICdj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "379fa6f8-2386-4e19-e15a-28937031468e"
      },
      "source": [
        "text_pre_processor(document)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'hello hello hear hear python amazing language use scripting web development backend development information retrieval natural language processing machine learning artificial intelligence wait go get start learn learn already get headstart get python'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsIpekwoICdl"
      },
      "source": [
        "# Test on a corpus of documents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2S1H1zakICdo"
      },
      "source": [
        "corpus = [\"\"\"<p>Héllo! Héllo! can you hear me! I just heard about <b>Python</b>!<br/>\\r\\n \n",
        "              It's an amazing language which can be used for [Scripting\\tWeb development\\tBackend development],\\r\\n\\r\\n\n",
        "              Information Retrieval, Natural Language Processing, Machine Learning & Artificial Intelligence!\\n\n",
        "              What are you waiting for? Go and get started.<br/> He's learning, she's learning, they've already\\n\\n\n",
        "              got a headstart! GET PYTHON 3.6 NOW!</p>\n",
        "           \"\"\",\n",
        "          \"\"\"US unveils world's most powerful supercomputer, beats China. \n",
        "             The US has unveiled the world's most powerful supercomputer \n",
        "             called 'Summit', beating the previous record-holder China's Sunway \n",
        "             TaihuLight. With a peak performance of 200,000 trillion calculations \n",
        "             per second, it is over twice as fast as Sunway TaihuLight, which is capable \n",
        "             of 93,000 trillion calculations per second. Summit has 4,608 servers, \n",
        "             which reportedly take up the size of two tennis courts.\"\"\",\n",
        "          \"\"\"The Lord of the Rings is an epic high fantasy novel written by English author and scholar J. R. R. Tolkien. \n",
        "            The story began as a sequel to Tolkien's 1937 fantasy novel The Hobbit, but eventually developed into \n",
        "            a much larger work. Written in stages between 1937 and 1949, The Lord of the Rings is one of the \n",
        "            best-selling novels ever written, with over 150 million copies sold.[1]\n",
        "          \"\"\",\n",
        "          \"\"\"The title of the novel refers to the story's main antagonist, the Dark Lord Sauron,[a] \n",
        "             who had in an earlier age created the One Ring to rule the other Rings of Power as the ultimate weapon \n",
        "             in his campaign to conquer and rule all of Middle-earth. From quiet beginnings in the Shire, a hobbit \n",
        "             land not unlike the English countryside, the story ranges across Middle-earth, following the course \n",
        "             of the War of the Ring through the eyes of its characters, not only the hobbits Frodo Baggins, \n",
        "             Samwise \"Sam\" Gamgee, Meriadoc \"Merry\" Brandybuck and Peregrin \"Pippin\" Took, but also the hobbits' \n",
        "             chief allies and travelling companions: the Men, Aragorn, a Ranger of the North, and Boromir, \n",
        "             a Captain of Gondor; Gimli, a Dwarf warrior; Legolas Greenleaf, an Elven prince; and Gandalf, a wizard.\n",
        "          \"\"\"\n",
        "]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rj7oRS8hQYco",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1107ff14-b7b0-43bd-9ef3-b227b78de28d"
      },
      "source": [
        "norm_docs = corpus_pre_processor(corpus)\n",
        "norm_docs[:2]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 34.50it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hello hello hear hear python amazing language use scripting web development backend development information retrieval natural language processing machine learning artificial intelligence wait go get start learn learn already get headstart get python',\n",
              " 'us unveil world powerful supercomputer beat china us unveil world powerful supercomputer call summit beat previous record holder china sunway taihulight peak performance trillion calculation per second twice fast sunway taihulight capable trillion calculation per second summit server reportedly take size two tennis court']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZRQDfTTQpzh"
      },
      "source": [
        "# Optional: Pre-processor with multi-threading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyYMPuirQbj6"
      },
      "source": [
        "from concurrent import futures\n",
        "import threading\n",
        "\n",
        "def parallel_preprocessing(idx, doc, total_docs):\n",
        "    return text_pre_processor(doc)\n",
        "\n",
        "\n",
        "def pre_process_documents_parallel(documents):\n",
        "    total_docs = len(documents)\n",
        "    docs_input = [[idx, doc, total_docs] for idx, doc in enumerate(documents)]\n",
        "    \n",
        "    ex = futures.ThreadPoolExecutor(max_workers=None)\n",
        "    print('preprocessing: starting')\n",
        "    norm_descriptions_map = ex.map(parallel_preprocessing, \n",
        "                                   [record[0] for record in docs_input],\n",
        "                                   [record[1] for record in docs_input],\n",
        "                                   [record[2] for record in docs_input])\n",
        "    norm_descriptions = list(norm_descriptions_map)\n",
        "    return norm_descriptions"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFBm2MmZRck3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed532f9e-917a-4198-b09d-86482bd8a233"
      },
      "source": [
        "norm_docs = pre_process_documents_parallel(corpus)\n",
        "norm_docs"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "preprocessing: starting\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hello hello hear hear python amazing language use scripting web development backend development information retrieval natural language processing machine learning artificial intelligence wait go get start learn learn already get headstart get python',\n",
              " 'us unveil world powerful supercomputer beat china us unveil world powerful supercomputer call summit beat previous record holder china sunway taihulight peak performance trillion calculation per second twice fast sunway taihulight capable trillion calculation per second summit server reportedly take size two tennis court',\n",
              " 'lord rings epic high fantasy novel write english author scholar j r r tolkien story begin sequel tolkien fantasy novel hobbit eventually develop much large work write stage lord rings one best sell novel ever write million copy sold',\n",
              " 'title novel refer story main antagonist dark lord saurona early age create one ring rule rings power ultimate weapon campaign conquer rule middle earth quiet beginning shire hobbit land unlike english countryside story range across middle earth follow course war ring eye character hobbit frodo baggins samwise sam gamgee meriadoc merry brandybuck peregrin pippin take also hobbit chief ally travel companion men aragorn ranger north boromir captain gondor gimli dwarf warrior legolas greenleaf elven prince gandalf wizard']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    }
  ]
}